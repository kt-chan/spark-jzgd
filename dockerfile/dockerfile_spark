FROM hadoop:stable

# System packages
# RUN apt-get clean && apt-get update -y
# RUN apt-get install -y python3 python3-pip curl wget unzip procps openjdk-8-jdk libpostgresql-jdbc-java && \
#   ln -s /usr/bin/python3 /usr/bin/python && \
#   rm -rf /var/lib/apt/lists/*
# 
# RUN pip3 install wget requests datawrangler
# RUN apt-get install -y wget procps libpostgresql-jdbc-java 

# Versions
ENV OPENJDK_VERSION=8
ENV BUILD_DATE=2023-01-16
ENV SPARK_VERSION=3.3.1
ENV HADOOP_VERSION=3.2.3
ENV HIVE_VERSION=3.1.2
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/usr/bin/hadoop-${HADOOP_VERSION}
ENV SPARK_HOME=/usr/bin/spark-${SPARK_VERSION}
ENV HDFS_HOME=/user/hdfs
ENV HIVE_HOME=/usr/bin/hive-${HIVE_VERSION}

LABEL org.label-schema.name="Apache Spark ${SPARK_VERSION}" \
      org.label-schema.build-date=$BUILD_DATE \
      org.label-schema.version=$SPARK_VERSION      


# Download
# RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz"

# Import
COPY ./tars/spark-${SPARK_VERSION}-bin-hadoop3.tgz .
RUN tar xzf "spark-${SPARK_VERSION}-bin-hadoop3.tgz" && \
    rm "spark-${SPARK_VERSION}-bin-hadoop3.tgz" && \
    mv "spark-${SPARK_VERSION}-bin-hadoop3" $SPARK_HOME && \
	apt-get clean

	
RUN curl -o ${SPARK_HOME}/jars/postgresql-jdbc4.jar  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar
    

# Add User
# RUN groupadd -r hadoop --gid=1001 
# RUN useradd -m hdfs -g hadoop && adduser hdfs sudo && echo "hdfs     ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers
# RUN useradd -m hive -g hadoop && adduser hive sudo && echo "hive     ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers
RUN useradd -m spark -g hadoop && adduser spark sudo && echo "spark     ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers


# Prepare dirs
RUN mkdir -p /tmp/logs/ && chmod a+w /tmp/logs/ && mkdir /app && chmod a+rwx /app 

# update spark-default.conf
COPY ./conf/spark-defaults.conf $SPARK_HOME/conf/spark-defaults.conf
COPY ./conf/hdfs-site.xml $SPARK_HOME/conf/hdfs-site.xml
COPY ./conf/hive-site.xml $SPARK_HOME/conf/hive-site.xml
COPY ./conf/log4j.properties $SPARK_HOME/conf/log4j.properties

WORKDIR ${SPARK_HOME}

COPY ./scripts/runMaster.sh  /usr/local/bin
RUN chown spark:hadoop /usr/local/bin/runMaster.sh && chmod +x /usr/local/bin/runMaster.sh

COPY ./scripts/runWorker.sh  /usr/local/bin
RUN chown spark:hadoop /usr/local/bin/runWorker.sh && chmod +x /usr/local/bin/runWorker.sh

COPY ./scripts/runThrift.sh  /usr/local/bin
RUN chown spark:hadoop /usr/local/bin/runThrift.sh && chmod +x /usr/local/bin/runThrift.sh

COPY ./tars/hadoop-aws-${SPARK_VERSION}.jar ${SPARK_HOME}/jars
COPY ./tars/aws-java-sdk-bundle-1.12.383.jar ${SPARK_HOME}/jars
RUN chown spark:hadoop ${SPARK_HOME}/jars/hadoop-aws-${SPARK_VERSION}.jar
RUN chown spark:hadoop ${SPARK_HOME}/jars/aws-java-sdk-bundle-1.12.383.jar

RUN mkdir -p  ${SPARK_HOME}/logs
RUN chown spark:hadoop -R ${SPARK_HOME}

#Set up ssh login
COPY ./conf/ssh_config /etc/ssh/ssh_config
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys

ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64
ENV PATH $PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
RUN export PATH=$PATH
RUN echo $PATH
RUN hadoop classpath

# SET ENVIRONMENT VARIABLES
RUN echo "export SPARK_DIST_CLASSPATH=$(hadoop classpath)" >> $SPARK_HOME/conf/spark-env.sh
RUN chown spark:hadoop $SPARK_HOME/conf/spark-env.sh

RUN echo "export JAVA_HOME=$JAVA_HOME" >> ~/.bashrc
RUN echo "export SPARK_HOME=$SPARK_HOME" >> ~/.bashrc
RUN echo "export HADOOP_HOME=$HADOOP_HOME" >> ~/.bashrc
RUN echo "export PATH=$PATH" >> ~/.bashrc

expose 4040 7070 8081 8082 10000 22