{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --upgrade pip\n",
    "!pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:03:42,681] root {<ipython-input-1-c95290b1fcef>:22} INFO - Using following scale configuration: \n",
      "[2023-05-19 12:03:42,686] root {<ipython-input-1-c95290b1fcef>:24} INFO - scale_gb:1\n",
      "[2023-05-19 12:03:42,687] root {<ipython-input-1-c95290b1fcef>:24} INFO - batch_k:100\n",
      "[2023-05-19 12:03:42,692] root {<ipython-input-1-c95290b1fcef>:24} INFO - timespan_days:7\n",
      "[2023-05-19 12:03:42,694] root {<ipython-input-1-c95290b1fcef>:24} INFO - droptable:True\n",
      "[2023-05-19 12:03:42,695] root {<ipython-input-1-c95290b1fcef>:24} INFO - debugmode:True\n",
      "[2023-05-19 12:03:42,696] root {<ipython-input-1-c95290b1fcef>:37} INFO - Using following system configuration: \n",
      "[2023-05-19 12:03:42,698] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark.app.name:\"Remote Spark\"\n",
      "[2023-05-19 12:03:42,699] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark.master.hostpath:\"spark://spark-master:7077\"\n",
      "[2023-05-19 12:03:42,700] root {<ipython-input-1-c95290b1fcef>:39} INFO - hive.metastore.uris:\"thrift://hms-service:9083\"\n",
      "[2023-05-19 12:03:42,701] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark.sql.warehouse.dir:\"hdfs://hadoop-service:9000/user/hive/warehouse\"\n",
      "[2023-05-19 12:03:42,702] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark_driver_cores:2\n",
      "[2023-05-19 12:03:42,703] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark.driver.memory:2G\n",
      "[2023-05-19 12:03:42,703] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark.executor.cores:1\n",
      "[2023-05-19 12:03:42,704] root {<ipython-input-1-c95290b1fcef>:39} INFO - spark.executor.memory:1G\n"
     ]
    }
   ],
   "source": [
    "import sys,os,logging\n",
    "import argparse\n",
    "\n",
    "date_strftime_format = \"%d-%b-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='[%(asctime)s] %(name)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--f\", \"--filepath\", type=str, default=\"./conf/default.conf\", help='provide configuration filepath')\n",
    "args = parser.parse_args(args=['--filepath', './conf/default.conf'])\n",
    "configFilePath = args.f\n",
    "\n",
    "from configparser import ConfigParser\n",
    "config_object = ConfigParser()\n",
    "config_object.read(configFilePath)\n",
    "scaleinfo = config_object[\"SCALEINFO\"]\n",
    "scale = scaleinfo.getint(\"scale_gb\")\n",
    "batch = scaleinfo.getint(\"batch_k\")\n",
    "timespan_days = scaleinfo.getint(\"timespan_days\")\n",
    "droptable = scaleinfo.getboolean(\"droptable\")\n",
    "debugMode = scaleinfo.getboolean(\"debugMode\")\n",
    "\n",
    "logging.info(\"Using following scale configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SCALEINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)\n",
    "\n",
    "    \n",
    "systeminfo = config_object[\"SYSTEMINFO\"]\n",
    "SPARK_APP_NAME = str(systeminfo.get(\"spark.app.name\")).strip('\\\"')\n",
    "SPARK_MASTER = str(systeminfo.get(\"spark.master.hostpath\")).strip('\\\"')\n",
    "HIVE_HMS_HOST= str(systeminfo.get(\"hive.metastore.uris\")).strip('\\\"')\n",
    "SPARK_WAREHOUSE_DIR = str(systeminfo.get(\"spark.sql.warehouse.dir\")).strip('\\\"')\n",
    "SPARK_DRIVER_CORES = systeminfo.getint(\"spark_driver_cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.driver.memory\")).strip('\\\"')\n",
    "SPARK_EXECUTOR_CORES = systeminfo.getint(\"spark.executor.cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.executor.memory\")).strip('\\\"')\n",
    "\n",
    "logging.info(\"Using following system configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SYSTEMINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:03:43,387] numexpr.utils {utils.py:141} INFO - NumExpr defaulting to 8 threads.\n",
      "[2023-05-19 12:03:50,434] root {<ipython-input-2-862c62e33add>:26} INFO - Spark Version: 3.3.1\n",
      "[2023-05-19 12:03:50,443] root {<ipython-input-2-862c62e33add>:27} INFO - PySpark Version: 3.3.1\n",
      "[2023-05-19 12:03:50,448] root {<ipython-input-2-862c62e33add>:28} INFO - Pandas Version: 1.3.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(SPARK_APP_NAME) \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"hive.metastore.uris\", HIVE_HMS_HOST) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", SPARK_WAREHOUSE_DIR) \\\n",
    "        .config(\"spark_driver_cores\", SPARK_DRIVER_CORES) \\\n",
    "        .config(\"spark.driver.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .config(\"spark.executor.cores\", SPARK_EXECUTOR_CORES) \\\n",
    "        .config(\"spark.executor.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "sqlContext = SQLContext(spark.sparkContext, sparkSession=spark)\n",
    "spark.sparkContext.version\n",
    "logging.info(\"Spark Version: \" + spark.version)\n",
    "logging.info(\"PySpark Version: \" + pyspark.__version__)\n",
    "logging.info(\"Pandas Version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:03:50,805] root {<ipython-input-3-de35c3d606fb>:39} INFO - Start\n",
      "[2023-05-19 12:03:50,808] root {<ipython-input-3-de35c3d606fb>:48} INFO - dropping sample.* tables\n",
      "[2023-05-19 12:03:55,821] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:00,829] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:05,845] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:10,856] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:11,327] root {<ipython-input-3-de35c3d606fb>:58} INFO - deleting hdfs directory: /user/hive/warehouse/sample.db\n",
      "[2023-05-19 12:04:11,371] root {<ipython-input-3-de35c3d606fb>:33} INFO - deleting hdfs directory: hdfs://hadoop-service:9000/user/hive/warehouse/sample.db\n",
      "[2023-05-19 12:04:11,391] root {<ipython-input-3-de35c3d606fb>:36} INFO - deleted /user/hive/warehouse/sample.db\n",
      "[2023-05-19 12:04:11,511] root {<ipython-input-3-de35c3d606fb>:233} INFO - number_of_workers: 6.\n",
      "[2023-05-19 12:04:11,512] root {<ipython-input-3-de35c3d606fb>:199} WARNING - Testing with 2 rounds data instead of 41 rounds, remove this by settign debugMode = False in config file.\n",
      "[2023-05-19 12:04:11,513] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _i3: 12.0 KiB\n",
      "[2023-05-19 12:04:11,514] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _ii:  1.8 KiB\n",
      "[2023-05-19 12:04:11,515] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _i1:  1.8 KiB\n",
      "[2023-05-19 12:04:11,515] root {<ipython-input-3-de35c3d606fb>:210} INFO -                   ConfigParser:  1.4 KiB\n",
      "[2023-05-19 12:04:11,516] root {<ipython-input-3-de35c3d606fb>:210} INFO -                     SQLContext:  1.0 KiB\n",
      "[2023-05-19 12:04:11,517] root {<ipython-input-3-de35c3d606fb>:210} INFO -                   SparkSession:  1.0 KiB\n",
      "[2023-05-19 12:04:11,518] root {<ipython-input-3-de35c3d606fb>:210} INFO -                             _i:  1.0 KiB\n",
      "[2023-05-19 12:04:11,519] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _i2:  1.0 KiB\n",
      "[2023-05-19 12:04:11,520] root {<ipython-input-3-de35c3d606fb>:210} INFO -                       alphabet:  672.0 B\n",
      "[2023-05-19 12:04:11,521] root {<ipython-input-3-de35c3d606fb>:210} INFO -                           cols:  320.0 B\n",
      "[2023-05-19 12:04:11,522] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     current_ts:  128.0 B\n",
      "[2023-05-19 12:04:11,522] root {<ipython-input-3-de35c3d606fb>:212} INFO -                           name:   53.0 B\n",
      "[2023-05-19 12:04:11,523] root {<ipython-input-3-de35c3d606fb>:212} INFO -                          scale:   28.0 B\n",
      "[2023-05-19 12:04:11,524] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     batch_size:   28.0 B\n",
      "[2023-05-19 12:04:11,525] root {<ipython-input-3-de35c3d606fb>:212} INFO -                  timespan_days:   28.0 B\n",
      "[2023-05-19 12:04:11,526] root {<ipython-input-3-de35c3d606fb>:212} INFO -                   scale_factor:   28.0 B\n",
      "[2023-05-19 12:04:11,526] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     scale_unit:   28.0 B\n",
      "[2023-05-19 12:04:11,527] root {<ipython-input-3-de35c3d606fb>:212} INFO -                   scale_rounds:   28.0 B\n",
      "[2023-05-19 12:04:11,528] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     partitions:   28.0 B\n",
      "[2023-05-19 12:04:11,529] root {<ipython-input-3-de35c3d606fb>:212} INFO -                           size:   28.0 B\n",
      "[2023-05-19 12:04:11,531] root {<ipython-input-3-de35c3d606fb>:91} INFO - Round 1: generating noise data for tb_test with batch size 100000 rows, from 1683893051 to 1684195451.\n",
      "[2023-05-19 12:04:15,864] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:19,060] root {<ipython-input-3-de35c3d606fb>:115} INFO - Memory Usage: 137,900,128 Bytes\n",
      "[2023-05-19 12:04:19,063] root {<ipython-input-3-de35c3d606fb>:117} INFO - creating spark data frame for partitioins from 2023-05-12 12:04:11 to 2023-05-16 00:04:10... \n",
      "[2023-05-19 12:04:20,894] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:25,924] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:30,943] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:35,966] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:40,996] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:44,492] root {<ipython-input-3-de35c3d606fb>:124} INFO - writing to hive table sample.tb_test ...\n",
      "[2023-05-19 12:04:46,018] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:51,021] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:04:56,029] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:01,035] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:06,040] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:11,044] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:16,052] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:21,059] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:26,077] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:29,370] root {<ipython-input-3-de35c3d606fb>:133} INFO - finished round 1 with batch size 100000 rows with 44 seconds\n",
      "[2023-05-19 12:05:29,408] root {<ipython-input-3-de35c3d606fb>:144} INFO - Round 1: generating target data for tb_test with batch size 10000 rows, from 1683893051 to 1684195451.\n",
      "[2023-05-19 12:05:30,216] root {<ipython-input-3-de35c3d606fb>:171} INFO - Memory Usage: 13,790,128 Bytes\n",
      "[2023-05-19 12:05:30,218] root {<ipython-input-3-de35c3d606fb>:173} INFO - creating spark data frame for partitioins from 2023-05-12 12:04:51 to 2023-05-16 00:01:45... \n",
      "[2023-05-19 12:05:31,094] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:33,287] root {<ipython-input-3-de35c3d606fb>:181} INFO - writing to hive table sample.tb_test ...\n",
      "[2023-05-19 12:05:36,113] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:41,121] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:46,124] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:48,940] root {<ipython-input-3-de35c3d606fb>:188} INFO - finished round 1 with batch size 10000 rows with 15 seconds\n",
      "[2023-05-19 12:05:48,946] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _i3: 12.0 KiB\n",
      "[2023-05-19 12:05:48,949] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _ii:  1.8 KiB\n",
      "[2023-05-19 12:05:48,950] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _i1:  1.8 KiB\n",
      "[2023-05-19 12:05:48,951] root {<ipython-input-3-de35c3d606fb>:210} INFO -                   ConfigParser:  1.4 KiB\n",
      "[2023-05-19 12:05:48,952] root {<ipython-input-3-de35c3d606fb>:210} INFO -                     SQLContext:  1.0 KiB\n",
      "[2023-05-19 12:05:48,952] root {<ipython-input-3-de35c3d606fb>:210} INFO -                   SparkSession:  1.0 KiB\n",
      "[2023-05-19 12:05:48,953] root {<ipython-input-3-de35c3d606fb>:210} INFO -                             _i:  1.0 KiB\n",
      "[2023-05-19 12:05:48,954] root {<ipython-input-3-de35c3d606fb>:210} INFO -                            _i2:  1.0 KiB\n",
      "[2023-05-19 12:05:48,955] root {<ipython-input-3-de35c3d606fb>:210} INFO -                       alphabet:  672.0 B\n",
      "[2023-05-19 12:05:48,956] root {<ipython-input-3-de35c3d606fb>:210} INFO -                           cols:  320.0 B\n",
      "[2023-05-19 12:05:48,957] root {<ipython-input-3-de35c3d606fb>:212} INFO -                      target_ts:  128.0 B\n",
      "[2023-05-19 12:05:48,958] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     current_ts:  128.0 B\n",
      "[2023-05-19 12:05:48,958] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     initial_ts:  128.0 B\n",
      "[2023-05-19 12:05:48,959] root {<ipython-input-3-de35c3d606fb>:212} INFO -                           name:   53.0 B\n",
      "[2023-05-19 12:05:48,960] root {<ipython-input-3-de35c3d606fb>:212} INFO -                          scale:   28.0 B\n",
      "[2023-05-19 12:05:48,961] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     batch_size:   28.0 B\n",
      "[2023-05-19 12:05:48,962] root {<ipython-input-3-de35c3d606fb>:212} INFO -                  timespan_days:   28.0 B\n",
      "[2023-05-19 12:05:48,962] root {<ipython-input-3-de35c3d606fb>:212} INFO -                   scale_factor:   28.0 B\n",
      "[2023-05-19 12:05:48,963] root {<ipython-input-3-de35c3d606fb>:212} INFO -                     scale_unit:   28.0 B\n",
      "[2023-05-19 12:05:48,964] root {<ipython-input-3-de35c3d606fb>:212} INFO -                   scale_rounds:   28.0 B\n",
      "[2023-05-19 12:05:48,966] root {<ipython-input-3-de35c3d606fb>:91} INFO - Round 2: generating noise data for tb_test with batch size 100000 rows, from 1684195451 to 1684497851.\n",
      "[2023-05-19 12:05:51,136] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:05:55,972] root {<ipython-input-3-de35c3d606fb>:115} INFO - Memory Usage: 136,700,128 Bytes\n",
      "[2023-05-19 12:05:55,975] root {<ipython-input-3-de35c3d606fb>:117} INFO - creating spark data frame for partitioins from 2023-05-16 00:04:11 to 2023-05-19 12:04:08... \n",
      "[2023-05-19 12:05:56,248] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:01,261] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:06,274] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:11,292] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:16,307] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:20,585] root {<ipython-input-3-de35c3d606fb>:124} INFO - writing to hive table sample.tb_test ...\n",
      "[2023-05-19 12:06:21,317] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:26,320] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:31,328] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:36,335] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:40,748] root {<ipython-input-3-de35c3d606fb>:133} INFO - finished round 2 with batch size 100000 rows with 20 seconds\n",
      "[2023-05-19 12:06:40,782] root {<ipython-input-3-de35c3d606fb>:144} INFO - Round 2: generating target data for tb_test with batch size 10000 rows, from 1684195451 to 1684497851.\n",
      "[2023-05-19 12:06:41,348] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:41,529] root {<ipython-input-3-de35c3d606fb>:171} INFO - Memory Usage: 13,790,128 Bytes\n",
      "[2023-05-19 12:06:41,531] root {<ipython-input-3-de35c3d606fb>:173} INFO - creating spark data frame for partitioins from 2023-05-16 00:05:16 to 2023-05-19 12:03:51... \n",
      "[2023-05-19 12:06:43,954] root {<ipython-input-3-de35c3d606fb>:181} INFO - writing to hive table sample.tb_test ...\n",
      "[2023-05-19 12:06:46,364] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:51,369] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:06:56,372] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:00,561] root {<ipython-input-3-de35c3d606fb>:188} INFO - finished round 2 with batch size 10000 rows with 16 seconds\n",
      "[2023-05-19 12:07:00,566] root {<ipython-input-3-de35c3d606fb>:226} INFO - generating data for tb_servu\n",
      "[2023-05-19 12:07:00,567] root {<ipython-input-3-de35c3d606fb>:68} INFO - generating data for tb_sev_u ...\n",
      "[2023-05-19 12:07:01,377] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:06,385] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:11,393] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:16,397] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:21,401] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:26,406] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:31,413] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:36,421] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:41,429] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:46,438] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:49,017] root {<ipython-input-3-de35c3d606fb>:80} INFO - finished loading data for tb_sev_u with 100000 rows.\n",
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|   sample|           tb_sev_u|      false|\n",
      "|   sample|            tb_test|      false|\n",
      "|   sample|        tb_test_num|      false|\n",
      "|   sample|tb_test_qf_stat_log|      false|\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "[2023-05-19 12:07:49,683] py4j.clientserver {clientserver.py:543} INFO - Closing down clientserver connection\n",
      "[2023-05-19 12:07:51,445] root {<ipython-input-3-de35c3d606fb>:42} INFO - .\n",
      "[2023-05-19 12:07:51,446] root {<ipython-input-3-de35c3d606fb>:256} INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "taskdone = False\n",
    "alphabet = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "LENGTH = 10\n",
    "\n",
    "cols=[\"f01\", \"f02\",\"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\"f18\",\"f19\",\"f20\",\"f21\",\"f22\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\",\"f30\",\"f31\" ]\n",
    "random_txtcols=[\"f03\",\"f05\",\"f08\",\"f09\",\"f11\",\"f12\",\"f13\",\"f16\",\"f17\",\"f18\",\"f20\",\"f21\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\"]\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def getNumberOfExecutor():\n",
    "    global spark\n",
    "    sc=spark.sparkContext\n",
    "    number_of_workers = len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "    return number_of_workers\n",
    "\n",
    "def delete_path(host, path):\n",
    "    global spark\n",
    "    sc=spark.sparkContext\n",
    "    URI           = sc._gateway.jvm.java.net.URI\n",
    "    Path          = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem    = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "    fs = FileSystem.get(URI(host), Configuration())\n",
    "    logging.info(\"deleting hdfs directory: \" + host + path)\n",
    "    if fs.exists(Path(path)) :\n",
    "        fs.delete(Path(path), True)\n",
    "        logging.info(\"deleted \" + path)\n",
    "\n",
    "def progressbar():\n",
    "    logging.info('Start')\n",
    "    while not (taskdone):\n",
    "        sleep(5)\n",
    "        logging.info('.')\n",
    "\n",
    "def inittask(droptable = False):\n",
    "    global SPARK_WAREHOUSE_DIR\n",
    "    \n",
    "    if(droptable):\n",
    "        logging.info('dropping sample.* tables')\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_sev_u;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test_qf_stat;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test_num_tmp;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test_qf_lastest;\")\n",
    "        # Delete the HDFS directory for failure case\n",
    "        pos=SPARK_WAREHOUSE_DIR.index(\"/\", len(\"hdfs://\"))\n",
    "        hdfshost=SPARK_WAREHOUSE_DIR[:pos]\n",
    "        hdfsdir=SPARK_WAREHOUSE_DIR[pos:]+\"/sample.db\"\n",
    "        logging.info('deleting hdfs directory: ' + hdfsdir)\n",
    "        delete_path(hdfshost, hdfsdir)\n",
    "    \n",
    "    sqlContext.sql(\"create database if not exists sample;\")\n",
    "    sqlContext.sql(\"use sample;\")\n",
    "    \n",
    "    return True\n",
    "    \n",
    "\n",
    "def gendata_tbsevu(rounds = 1, batch_size = 100000):\n",
    "    logging.info(\"generating data for tb_sev_u ...\")\n",
    "    \n",
    "    sqlContext.sql(\"\"\"\n",
    "        create table sample.tb_sev_u \n",
    "        as\n",
    "        select distinct f02 as id, f03 as name\n",
    "        from sample.tb_test \n",
    "        where f02 < 19218983880 or f02 > 19218983890\n",
    "        ORDER BY RAND () \n",
    "        limit {batch}\n",
    "    ;\"\"\".format(batch=int(rounds*batch_size)))\n",
    "\n",
    "    logging.info(\"finished loading data for tb_sev_u with \" + str(batch_size) + \" rows.\")    \n",
    "    \n",
    "    \n",
    "def gendata_tbtest_noise(fromTS, toTS, scale_rounds = 1, batch_size = 100000):\n",
    "    global random_txtcols\n",
    "    scale_rounds = scale_rounds + 1\n",
    "    ## for 100,000 rows of data with 25MB Storage Data Without Replication\n",
    "    df = pd.DataFrame()   \n",
    "    initial_ts_int =  int(fromTS.value/10**9)\n",
    "    target_ts_int =  int(toTS.value/10**9)\n",
    "    \n",
    "    logging.info(\"Round \" + str(scale_rounds) + \": generating noise data for tb_test with batch size \"+ str(batch_size) + \" rows, from \" + str(initial_ts_int) + \" to \" +  str(target_ts_int) + \".\")\n",
    "                 \n",
    "    df1 = pd.DataFrame(np.random.randint(initial_ts_int, target_ts_int, size=(batch_size,1)),  columns=list([\"f01\"]))\n",
    "    df2 = pd.DataFrame(np.random.randint(19200000000,19200000000+20000000,size=(batch_size,2)), columns=list([\"f02\", \"f04\"])) \n",
    "    df6 = pd.DataFrame(np.random.randint(0, 10, size=(batch_size,5)),  columns=list([\"f06\", \"f07\", \"f10\", \"f15\", \"f19\"]))\n",
    "    df14 = pd.DataFrame(np.random.randint(48, 51, size=(batch_size,1)),  columns=list([\"f14\"]))\n",
    "    df22 = pd.DataFrame(np.random.randint(1, 10, size=(batch_size,1)),  columns=list([\"f22\"]))\n",
    "    df30 = pd.DataFrame(np.random.randint(initial_ts_int,target_ts_int,size=(batch_size,1)),  columns=list([\"f30\"]))\n",
    "    df31 = df30\n",
    "    df31 = df31.rename(columns={\"f30\": \"f31\"})\n",
    "    \n",
    "    for k in random_txtcols: \n",
    "        np_batchsize = None\n",
    "        if k == 'f16' :\n",
    "            np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, math.ceil(1 + LENGTH * (np.random.randint(1, 30) / 10))])\n",
    "        else :\n",
    "            np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, LENGTH])\n",
    "\n",
    "        df0 = pd.DataFrame( [\"\".join(np_batchsize[i]) for i in range(len(np_batchsize))], columns=[k])\n",
    "        df[k] = df0[k]\n",
    "\n",
    "    df = pd.concat([df1,df2, df6, df14, df22, df30, df31, df], axis=1, join='inner')\n",
    "\n",
    "    df = df[cols]\n",
    "    logging.info(\"Memory Usage: \" + f'{df.memory_usage(deep=True).sum():,}'  + \" Bytes\")\n",
    "\n",
    "    logging.info(\"creating spark data frame for partitioins from \" + str(pd.to_datetime(df['f01'].min(), unit='s')) + ' to ' +  str(pd.to_datetime(df['f01'].max(), unit='s')) + '... ')\n",
    "    sparkDF = spark.createDataFrame(df)\n",
    "    sparkDF = sparkDF.withColumn(\"cp\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    sparkDF = sparkDF.withColumn(\"ld\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    sparkDF = sparkDF.withColumn(\"f30\", from_unixtime(sparkDF[\"f30\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    sparkDF = sparkDF.withColumn(\"f31\", from_unixtime(sparkDF[\"f31\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    #sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).bucketBy(32, \"f02\").sortBy(\"f01\").format(\"orc\").saveAsTable(\"tb_test\")\n",
    "    logging.info(\"writing to hive table sample.tb_test ...\")\n",
    "    \n",
    "    current_ts = pd.Timestamp.now()\n",
    "    \n",
    "    sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).format(\"orc\").option(\"compression\",\"ZLIB\").saveAsTable(\"sample.tb_test\")\n",
    "    finish_ts = pd.Timestamp.now()\n",
    "    \n",
    "    spark.catalog.clearCache()\n",
    "    gc.collect()\n",
    "    logging.info(\"finished round \" + str(scale_rounds) +  \" with batch size \"+ str(batch_size) + \" rows with \" + str( (finish_ts - current_ts).seconds)  + \" seconds\")\n",
    "        \n",
    "\n",
    "def gendata_tbtest_target(fromTS, toTS, scale_rounds = 1, batch_size = 100000):\n",
    "    global random_txtcols\n",
    "    scale_rounds = scale_rounds + 1\n",
    "    ## for 100,000 rows of data with 25MB Storage Data Without Replication\n",
    "    df = pd.DataFrame()   \n",
    "    initial_ts_int =  int(fromTS.value/10**9)\n",
    "    target_ts_int =  int(toTS.value/10**9)\n",
    "\n",
    "    logging.info(\"Round \" + str(scale_rounds) + \": generating target data for tb_test with batch size \"+ str(batch_size) + \" rows, from \" + str(initial_ts_int) + \" to \" +  str(target_ts_int) + \".\")\n",
    "                 \n",
    "    df1 = pd.DataFrame(np.random.randint(initial_ts_int, target_ts_int, size=(batch_size,1)),  columns=list([\"f01\"]))\n",
    "    df2 = pd.DataFrame(np.random.randint(19218983880,19218983880+10,size=(batch_size,1)), columns=list([\"f02\"])) \n",
    "    df4 = pd.DataFrame(np.random.randint(19200000000,19200000000+20000000,size=(batch_size,1)), columns=list([\"f04\"])) \n",
    "    df6 = pd.DataFrame(0, index=range(batch_size),  columns=list([\"f06\"]))\n",
    "    df7 = pd.DataFrame(7, index=range(batch_size),  columns=list([\"f07\"]))\n",
    "    df10 = pd.DataFrame(np.random.randint(0, 10, size=(batch_size,3)),  columns=list([\"f10\", \"f15\", \"f19\"]))\n",
    "    df14 = pd.DataFrame(49, index=range(batch_size),  columns=list([\"f14\"]))\n",
    "    df22 = pd.DataFrame(3, index=range(batch_size),  columns=list([\"f22\"]))\n",
    "    df30 = pd.DataFrame(np.random.randint(initial_ts_int,target_ts_int,size=(batch_size,1)),  columns=list([\"f30\"]))\n",
    "    df31 = df30\n",
    "    df31 = df31.rename(columns={\"f30\": \"f31\"})\n",
    "\n",
    "    for k in random_txtcols: \n",
    "        if k == 'f16' :\n",
    "            np_batchsize =  pd.DataFrame(\"Great you found me !\", index=range(batch_size),  columns=list([\"f16\"]))\n",
    "        else :\n",
    "            np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, LENGTH])\n",
    "            np_batchsize = pd.DataFrame( [\"\".join(np_batchsize[i]) for i in range(len(np_batchsize))], columns=[k])\n",
    "\n",
    "        df[k] = np_batchsize\n",
    "\n",
    "    df = pd.concat([df1,df2, df4, df6, df7, df10, df14, df22, df30, df31, df], axis=1, join='inner')\n",
    "\n",
    "    df = df[cols]\n",
    "\n",
    "    logging.info(\"Memory Usage: \" + f'{df.memory_usage(deep=True).sum():,}'  + \" Bytes\")\n",
    "\n",
    "    logging.info(\"creating spark data frame for partitioins from \" + str(pd.to_datetime(df['f01'].min(), unit='s')) + ' to ' +  str(pd.to_datetime(df['f01'].max(), unit='s')) + '... ')\n",
    "    sparkDF = spark.createDataFrame(df)\n",
    "    sparkDF = sparkDF.withColumn(\"cp\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    sparkDF = sparkDF.withColumn(\"ld\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    sparkDF = sparkDF.withColumn(\"f30\", from_unixtime(sparkDF[\"f30\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    sparkDF = sparkDF.withColumn(\"f31\", from_unixtime(sparkDF[\"f31\"], \"yyyyMMddHH\").cast(\"BigInt\"))\n",
    "    #sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).bucketBy(32, \"f02\").sortBy(\"f01\").format(\"orc\").saveAsTable(\"tb_test\")\n",
    "    \n",
    "    logging.info(\"writing to hive table sample.tb_test ...\")\n",
    "    current_ts = pd.Timestamp.now()\n",
    "    sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).format(\"orc\").option(\"compression\",\"ZLIB\").saveAsTable(\"sample.tb_test\")\n",
    "    finish_ts = pd.Timestamp.now()\n",
    "    \n",
    "    spark.catalog.clearCache()\n",
    "    gc.collect()\n",
    "    logging.info(\"finished round \" + str(scale_rounds) +  \" with batch size \"+ str(batch_size) + \" rows with \" + str( (finish_ts - current_ts).seconds)  + \" seconds\")\n",
    "\n",
    "    \n",
    "def gendata_tbtest(scale = 1, batch_size = 100000, timespan_days = 31):\n",
    "    \n",
    "    scale_factor = scale*1024\n",
    "    scale_unit =  math.ceil(batch_size/(100000/25))\n",
    "    scale_rounds = math.ceil(scale_factor/scale_unit)\n",
    "    \n",
    "    #Testing\n",
    "    if debugMode :\n",
    "        logging.warning(\"Testing with \" + str(2) +\" rounds data instead of \" + str(scale_rounds) +\" rounds, remove this by settign debugMode = False in config file.\")\n",
    "        scale_rounds = 2\n",
    "        \n",
    "    target_ts=None\n",
    "    partitions = timespan_days * 24\n",
    "    current_ts = pd.Timestamp.now()\n",
    "    \n",
    "    for i in range(scale_rounds):\n",
    "        \n",
    "        if debugMode :            \n",
    "            for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(globals().items())), key= lambda x: -x[1])[:10]:\n",
    "                logging.info(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "            for name, size in sorted(((name, sys.getsizeof(value)) for name, value in list(locals().items())), key= lambda x: -x[1])[:10]:\n",
    "                logging.info(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "        \n",
    "        if target_ts is None:\n",
    "            initial_ts = pd.to_datetime(current_ts - pd.Timedelta(days=timespan_days))\n",
    "        else:\n",
    "            initial_ts = target_ts;\n",
    "\n",
    "        target_ts =  pd.to_datetime(initial_ts + pd.Timedelta(hours=math.ceil(partitions / scale_rounds)))\n",
    "\n",
    "        gendata_tbtest_noise(initial_ts, target_ts, i, batch_size)\n",
    "        gendata_tbtest_target(initial_ts, target_ts, i, math.ceil(batch_size/10))\n",
    "\n",
    "    # Finish data generation for tb_test\n",
    "    # Then we load the data into tb_serv_u\n",
    "    logging.info(\"generating data for tb_servu\")\n",
    "    gendata_tbsevu(scale_rounds, batch_size)\n",
    "    \n",
    "    sqlContext.sql(\"use sample;\")\n",
    "    sqlContext.sql(\"show tables\").show()\n",
    "\n",
    "def gendata(scale_gb = 1, batch_size = 100, timespan_days = 31):\n",
    "    logging.info(\"number_of_workers: \" + str(getNumberOfExecutor()) + \".\")\n",
    "    gendata_tbtest(scale_gb, batch_size * 1000, timespan_days )\n",
    "    \n",
    "    \n",
    "def longtask(scale_gb = 1, batch_size = 100, timespan_days = 31, droptTable = False):\n",
    "    inittask(droptTable)\n",
    "    gendata(scale_gb, batch_size, timespan_days)\n",
    "    global taskdone\n",
    "    taskdone = True\n",
    "\n",
    "# start the thread pool\n",
    "t1 = threading.Thread(target=progressbar)\n",
    "t2 = threading.Thread(target=longtask,  args=(scale, batch, timespan_days, droptable))\n",
    "\n",
    "# wait for all tasks to complete\n",
    "# start threads\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until threads finish their job\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:07:51,452] root {<ipython-input-4-e844652b705f>:1} INFO - ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS  ;\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|f01                         |bigint                                                          |null   |\n",
      "|f02                         |bigint                                                          |null   |\n",
      "|f03                         |string                                                          |null   |\n",
      "|f04                         |bigint                                                          |null   |\n",
      "|f05                         |string                                                          |null   |\n",
      "|f06                         |bigint                                                          |null   |\n",
      "|f07                         |bigint                                                          |null   |\n",
      "|f08                         |string                                                          |null   |\n",
      "|f09                         |string                                                          |null   |\n",
      "|f10                         |bigint                                                          |null   |\n",
      "|f11                         |string                                                          |null   |\n",
      "|f12                         |string                                                          |null   |\n",
      "|f13                         |string                                                          |null   |\n",
      "|f14                         |bigint                                                          |null   |\n",
      "|f15                         |bigint                                                          |null   |\n",
      "|f16                         |string                                                          |null   |\n",
      "|f17                         |string                                                          |null   |\n",
      "|f18                         |string                                                          |null   |\n",
      "|f19                         |bigint                                                          |null   |\n",
      "|f20                         |string                                                          |null   |\n",
      "|f21                         |string                                                          |null   |\n",
      "|f22                         |bigint                                                          |null   |\n",
      "|f23                         |string                                                          |null   |\n",
      "|f24                         |string                                                          |null   |\n",
      "|f25                         |string                                                          |null   |\n",
      "|f26                         |string                                                          |null   |\n",
      "|f27                         |string                                                          |null   |\n",
      "|f28                         |string                                                          |null   |\n",
      "|f29                         |string                                                          |null   |\n",
      "|f30                         |bigint                                                          |null   |\n",
      "|f31                         |bigint                                                          |null   |\n",
      "|cp                          |bigint                                                          |null   |\n",
      "|ld                          |bigint                                                          |null   |\n",
      "|# Partition Information     |                                                                |       |\n",
      "|# col_name                  |data_type                                                       |comment|\n",
      "|cp                          |bigint                                                          |null   |\n",
      "|ld                          |bigint                                                          |null   |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Database                    |sample                                                          |       |\n",
      "|Table                       |tb_test                                                         |       |\n",
      "|Owner                       |spark                                                           |       |\n",
      "|Created Time                |Fri May 19 12:05:27 HKT 2023                                    |       |\n",
      "|Last Access                 |UNKNOWN                                                         |       |\n",
      "|Created By                  |Spark 3.3.1                                                     |       |\n",
      "|Type                        |MANAGED                                                         |       |\n",
      "|Provider                    |orc                                                             |       |\n",
      "|Statistics                  |43813666 bytes, 220000 rows                                     |       |\n",
      "|Location                    |hdfs://hadoop-service:9000/user/hive/warehouse/sample.db/tb_test|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.orc.OrcSerde                       |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.orc.OrcInputFormat                 |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat                |       |\n",
      "|Storage Properties          |[compression=ZLIB]                                              |       |\n",
      "|Partition Provider          |Catalog                                                         |       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS  ;\")\n",
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS  ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_test;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:10:22,535] root {<ipython-input-5-b9eb15a1c29a>:1} INFO - ANALYZE TABLE sample.tb_sev_u COMPUTE STATISTICS FOR ALL COLUMNS  ;\n",
      "+----------------------------+-----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                        |comment|\n",
      "+----------------------------+-----------------------------------------------------------------+-------+\n",
      "|id                          |bigint                                                           |null   |\n",
      "|name                        |string                                                           |null   |\n",
      "|                            |                                                                 |       |\n",
      "|# Detailed Table Information|                                                                 |       |\n",
      "|Database                    |sample                                                           |       |\n",
      "|Table                       |tb_sev_u                                                         |       |\n",
      "|Owner                       |spark                                                            |       |\n",
      "|Created Time                |Fri May 19 12:07:01 HKT 2023                                     |       |\n",
      "|Last Access                 |UNKNOWN                                                          |       |\n",
      "|Created By                  |Spark 3.3.1                                                      |       |\n",
      "|Type                        |MANAGED                                                          |       |\n",
      "|Provider                    |hive                                                             |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1684469430]                               |       |\n",
      "|Statistics                  |4600000 bytes, 200000 rows                                       |       |\n",
      "|Location                    |hdfs://hadoop-service:9000/user/hive/warehouse/sample.db/tb_sev_u|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe               |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       |       |\n",
      "|Storage Properties          |[serialization.format=1]                                         |       |\n",
      "|Partition Provider          |Catalog                                                          |       |\n",
      "+----------------------------+-----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"ANALYZE TABLE sample.tb_sev_u COMPUTE STATISTICS FOR ALL COLUMNS  ;\")\n",
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_sev_u COMPUTE STATISTICS FOR ALL COLUMNS  ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_sev_u;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:10:31,557] root {<ipython-input-6-57986b9459e5>:1} INFO - create table if not exists sample.tb_test_qf_stat ...\n",
      "[2023-05-19 12:10:31,775] root {<ipython-input-6-57986b9459e5>:17} INFO - Executing query: \n",
      "\n",
      "        create table if not exists sample.tb_test_qf_stat(\n",
      "        f22 string,\n",
      "        f02 string,\n",
      "        f16 string,\n",
      "        cnt bigint,\n",
      "        f06    string,\n",
      "        f07    string\n",
      "        )partitioned by (bd string, ad bigint)\n",
      "        ;\n",
      "       \n",
      "[2023-05-19 12:10:32,588] root {<ipython-input-6-57986b9459e5>:19} INFO - Table sample.tb_test_qf_stat created.\n",
      "[2023-05-19 12:10:32,594] root {<ipython-input-6-57986b9459e5>:22} INFO - create table if not exists sample.tb_test_qf_stat_log ...\n",
      "[2023-05-19 12:10:33,474] root {<ipython-input-6-57986b9459e5>:33} INFO - Executing query: \n",
      "\n",
      "        create table if not exists sample.tb_test_qf_stat_log(\n",
      "        batchId    string\n",
      "        )\n",
      "        ;\n",
      "       \n",
      "[2023-05-19 12:10:33,706] root {<ipython-input-6-57986b9459e5>:35} INFO - Table sample.tb_test_qf_stat_log created.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"create table if not exists sample.tb_test_qf_stat ...\")\n",
    "    \n",
    "sqlContext.sql(\"use sample;\") \n",
    "sqlContext.sql(\"drop table if exists sample.tb_test_qf_stat\");\n",
    "\n",
    "stmt = \"\"\"\n",
    "        create table if not exists sample.tb_test_qf_stat(\n",
    "        f22 string,\n",
    "        f02 string,\n",
    "        f16 string,\n",
    "        cnt bigint,\n",
    "        f06    string,\n",
    "        f07    string\n",
    "        )partitioned by (bd string, ad bigint)\n",
    "        ;\n",
    "       \"\"\"\n",
    "logging.info(\"Executing query: \\n\" + stmt)\n",
    "sqlContext.sql(stmt)\n",
    "logging.info(\"Table sample.tb_test_qf_stat created.\")\n",
    "\n",
    "\n",
    "logging.info(\"create table if not exists sample.tb_test_qf_stat_log ...\")\n",
    "    \n",
    "sqlContext.sql(\"use sample;\") \n",
    "sqlContext.sql(\"drop table if exists sample.tb_test_qf_stat_log\");\n",
    "\n",
    "stmt = \"\"\"\n",
    "        create table if not exists sample.tb_test_qf_stat_log(\n",
    "        batchId    string\n",
    "        )\n",
    "        ;\n",
    "       \"\"\"\n",
    "logging.info(\"Executing query: \\n\" + stmt)\n",
    "sqlContext.sql(stmt)\n",
    "logging.info(\"Table sample.tb_test_qf_stat_log created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:10:33,967] root {<ipython-input-7-96d75c89d957>:3} INFO - CREATE OR REPLACE TEMPORARY VIEW tb_test_qf_tmp1  ...\n",
      "[2023-05-19 12:10:34,139] root {<ipython-input-7-96d75c89d957>:32} INFO - \n",
      "Executing query: \n",
      "\n",
      "        CREATE OR REPLACE TEMPORARY VIEW tb_test_qf_tmp1 \n",
      "        as\n",
      "        with tb_test_qf_tmp as (\n",
      "            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n",
      "            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\?+','\\?') ,'0+','0'),'[ \t]+',' ') as f16,\n",
      "            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd-HH')  as bd,\n",
      "            2023051712 as ad,\n",
      "            trim(f06) as f06,trim(f07) as f07\n",
      "            from tb_test t\n",
      "            where f31 >= 2023051212 and f31 < 2023051712 and f14 = '49'\n",
      "        ),\n",
      "        tb_test_qf_tmp1 as (\n",
      "        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n",
      "        a.f06,a.f07\n",
      "        from tb_test_qf_tmp a\n",
      "        left join tb_sev_u b on a.f02 = b.id\n",
      "        where a.f02 is not null and b.id is null\n",
      "        and length(a.f16) > 10\n",
      "        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n",
      "        having cnt > 10\n",
      "        )\n",
      "        select * from tb_test_qf_tmp1\n",
      "        ;\n",
      "    \n",
      "[2023-05-19 12:10:37,195] root {<ipython-input-7-96d75c89d957>:34} INFO - View tb_test_qf_tmp1 created.\n",
      "[2023-05-19 12:10:37,201] root {<ipython-input-7-96d75c89d957>:50} INFO - Executing query: alter table tb_test_qf_stat drop if exists partition (ad = 2023051712)\n",
      "[2023-05-19 12:10:38,047] root {<ipython-input-7-96d75c89d957>:52} INFO - table sample.tb_test_qf_stat partition 2023051712 dropped.\n",
      "[2023-05-19 12:10:38,129] root {<ipython-input-7-96d75c89d957>:61} INFO - \n",
      "Executing query: \n",
      "\n",
      " insert into sample.tb_test_qf_stat partition(bd, ad)\n",
      " select f22,f02,f16,cnt,f06,f07,bd,ad\n",
      " from tb_test_qf_tmp1\n",
      ";\n",
      "\n",
      "[2023-05-19 12:13:20,240] root {<ipython-input-7-96d75c89d957>:67} INFO - Finished tb_test_qf_stat Insert Query with 162 seconds\n",
      "[2023-05-19 12:13:20,243] root {<ipython-input-7-96d75c89d957>:76} INFO - \n",
      "Executing query: \n",
      "\n",
      " insert into sample.tb_test_qf_stat_log\n",
      " select 2023051712\n",
      " from tb_test_qf_tmp1\n",
      ";\n",
      "\n",
      "[2023-05-19 12:15:28,181] root {<ipython-input-7-96d75c89d957>:78} INFO - \n",
      "Finished query: \n",
      "\n",
      " insert into sample.tb_test_qf_stat_log\n",
      " select 2023051712\n",
      " from tb_test_qf_tmp1\n",
      ";\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_view_tb_test_qf_tmp1(batchId, dateFrom, dateTo):\n",
    "    \n",
    "    logging.info(\"CREATE OR REPLACE TEMPORARY VIEW tb_test_qf_tmp1  ...\")\n",
    "\n",
    "    sqlContext.sql(\"use sample;\") \n",
    "    query = \"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW tb_test_qf_tmp1 \n",
    "        as\n",
    "        with tb_test_qf_tmp as (\n",
    "            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n",
    "            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\\\?+','\\?') ,'0+','0'),'[ \\t]+',' ') as f16,\n",
    "            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd-HH')  as bd,\n",
    "            {batchId} as ad,\n",
    "            trim(f06) as f06,trim(f07) as f07\n",
    "            from tb_test t\n",
    "            where f31 >= {date1} and f31 < {date2} and f14 = '49'\n",
    "        ),\n",
    "        tb_test_qf_tmp1 as (\n",
    "        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n",
    "        a.f06,a.f07\n",
    "        from tb_test_qf_tmp a\n",
    "        left join tb_sev_u b on a.f02 = b.id\n",
    "        where a.f02 is not null and b.id is null\n",
    "        and length(a.f16) > 10\n",
    "        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n",
    "        having cnt > 10\n",
    "        )\n",
    "        select * from tb_test_qf_tmp1\n",
    "        ;\n",
    "    \"\"\".format(batchId=batchId, date1=dateFrom, date2=dateTo)\n",
    "\n",
    "    logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "    sqlContext.sql(query) \n",
    "    logging.info(\"View tb_test_qf_tmp1 created.\")\n",
    "\n",
    "    \n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "    \n",
    "today = datetime.now()\n",
    "date1  = today - relativedelta(days=7)\n",
    "date2  = today - relativedelta(days=2)\n",
    "batchId = date2.strftime(\"%Y%m%d%H\")\n",
    "dateFrom = date1.strftime(\"%Y%m%d%H\")\n",
    "dateTo = date2.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "create_view_tb_test_qf_tmp1(batchId, dateFrom, dateTo)\n",
    "\n",
    "query = \"alter table tb_test_qf_stat drop if exists partition (ad = {batchId})\".format(batchId=batchId)\n",
    "logging.info(\"Executing query: \" + query)\n",
    "sqlContext.sql(query);\n",
    "logging.info(\"table sample.tb_test_qf_stat partition \" + str(batchId) + \" dropped.\")\n",
    "\n",
    "sqlContext.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "query = \"\"\"\n",
    " insert into sample.tb_test_qf_stat partition(bd, ad)\n",
    " select f22,f02,f16,cnt,f06,f07,bd,ad\n",
    " from tb_test_qf_tmp1\n",
    ";\n",
    "\"\"\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "\n",
    "sqlContext.sql(\"set hive.exec.dynamic.partition.mode=strict\")\n",
    "logging.info(\"Finished tb_test_qf_stat Insert Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"\"\"\n",
    " insert into sample.tb_test_qf_stat_log\n",
    " select {batchId}\n",
    " from tb_test_qf_tmp1\n",
    ";\n",
    "\"\"\".format(batchId=str(batchId))\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "sqlContext.sql(query) \n",
    "logging.info(\"\\nFinished query: \\n\" + query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = sqlContext.sql(\"select * from tb_test_qf_tmp1 \");\n",
    "df.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 12:15:28,383] root {<ipython-input-8-2481303482d5>:3} INFO - \n",
      "Executing query: \n",
      "drop table if exists sample.tb_test_qf_lastest;\n",
      "[2023-05-19 12:15:28,421] root {<ipython-input-8-2481303482d5>:5} INFO - \n",
      "Finished query: \n",
      "drop table if exists sample.tb_test_qf_lastest;\n",
      "[2023-05-19 12:15:28,425] root {<ipython-input-8-2481303482d5>:14} INFO - \n",
      "Executing query: \n",
      "\n",
      "create table  if not exists tb_test_qf_lastest as\n",
      "select f22,f02,f16,cnt,bd,f06,f07 from\n",
      "( select *, row_number() over(partition by f02 order by bd desc) rn from sample.tb_test_qf_stat) t\n",
      "where t.rn =1;\n",
      "\n",
      "[2023-05-19 12:15:56,637] root {<ipython-input-8-2481303482d5>:18} INFO - Finished Query with 28 seconds\n",
      "[2023-05-19 12:15:56,641] root {<ipython-input-8-2481303482d5>:21} INFO - \n",
      "Executing query: \n",
      "select * from tb_test_qf_lastest;\n",
      "+---+-----------+--------------------+---+-------------+---+---+\n",
      "|f22|f02        |f16                 |cnt|bd           |f06|f07|\n",
      "+---+-----------+--------------------+---+-------------+---+---+\n",
      "|3  |19218983880|Great you found me !|16 |2023-05-17-08|0  |7  |\n",
      "|3  |19218983881|Great you found me !|13 |2023-05-17-09|0  |7  |\n",
      "|3  |19218983882|Great you found me !|13 |2023-05-17-10|0  |7  |\n",
      "|3  |19218983883|Great you found me !|15 |2023-05-17-11|0  |7  |\n",
      "|3  |19218983884|Great you found me !|17 |2023-05-17-11|0  |7  |\n",
      "|3  |19218983885|Great you found me !|15 |2023-05-17-11|0  |7  |\n",
      "|3  |19218983886|Great you found me !|12 |2023-05-17-11|0  |7  |\n",
      "|3  |19218983887|Great you found me !|11 |2023-05-17-10|0  |7  |\n",
      "|3  |19218983888|Great you found me !|16 |2023-05-17-11|0  |7  |\n",
      "|3  |19218983889|Great you found me !|13 |2023-05-17-11|0  |7  |\n",
      "+---+-----------+--------------------+---+-------------+---+---+\n",
      "\n",
      "[2023-05-19 12:15:57,239] root {<ipython-input-8-2481303482d5>:26} INFO - Finished Query with 0 seconds\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"use sample;\") \n",
    "query = \"drop table if exists sample.tb_test_qf_lastest;\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "sqlContext.sql(query) \n",
    "logging.info(\"\\nFinished query: \\n\" + query)\n",
    "\n",
    "query = \"\"\"\n",
    "create table  if not exists tb_test_qf_lastest as\n",
    "select f22,f02,f16,cnt,bd,f06,f07 from\n",
    "( select *, row_number() over(partition by f02 order by bd desc) rn from sample.tb_test_qf_stat) t\n",
    "where t.rn =1;\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"\"\"select * from tb_test_qf_lastest;\"\"\";\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "df = sqlContext.sql(query) \n",
    "df.show(100, False)\n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|tableName          |isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|sample   |tb_sev_u           |false      |\n",
      "|sample   |tb_test            |false      |\n",
      "|sample   |tb_test_num        |false      |\n",
      "|sample   |tb_test_qf_lastest |false      |\n",
      "|sample   |tb_test_qf_stat    |false      |\n",
      "|sample   |tb_test_qf_stat_log|false      |\n",
      "|         |tb_test_qf_tmp1    |true       |\n",
      "+---------+-------------------+-----------+\n",
      "\n",
      "[2023-05-19 12:15:58,040] root {<ipython-input-9-71dfdaf8b156>:6} INFO - \n",
      "Executing query: \n",
      "drop table if exists sample.tb_test_num_tmp;\n",
      "[2023-05-19 12:15:58,076] root {<ipython-input-9-71dfdaf8b156>:17} INFO - \n",
      "Executing query: \n",
      "\n",
      "    create table  if not exists sample.tb_test_num_tmp as\n",
      "    select f22, f02, min(bd) as f_date, max(bd) as l_date,f06,f07\n",
      "    from tb_test_qf_tmp1 \n",
      "    group by f22, f02,f06,f07;\n",
      "\n",
      "[2023-05-19 12:17:23,256] root {<ipython-input-9-71dfdaf8b156>:21} INFO - Finished Query with 85 seconds\n",
      "[2023-05-19 12:17:23,265] root {<ipython-input-9-71dfdaf8b156>:25} INFO - \n",
      "Executing query: \n",
      "drop table if exists sample.tb_test_num_tmp1;\n",
      "[2023-05-19 12:17:23,302] root {<ipython-input-9-71dfdaf8b156>:29} INFO - Finished Query with 0 seconds\n",
      "[2023-05-19 12:17:23,306] root {<ipython-input-9-71dfdaf8b156>:40} INFO - \n",
      "Executing query: \n",
      "\n",
      "    create table if not exists sample.tb_test_num\n",
      "    as\n",
      "    select t.f22, f02, min(f_date) as f_date, max(l_date) as l_date,f06,f07\n",
      "    from sample.tb_test_num_tmp as t\n",
      "    group by t.f22,f02,f06,f07\n",
      "    limit 1;\n",
      "\n",
      "[2023-05-19 12:17:23,526] root {<ipython-input-9-71dfdaf8b156>:44} INFO - Finished Query with 0 seconds\n",
      "[2023-05-19 12:17:23,530] root {<ipython-input-9-71dfdaf8b156>:47} INFO - \n",
      "Executing query: \n",
      "Truncate table sample.tb_test_num;\n",
      "[2023-05-19 12:17:23,886] root {<ipython-input-9-71dfdaf8b156>:51} INFO - Finished Query with 0 seconds\n",
      "[2023-05-19 12:17:23,890] root {<ipython-input-9-71dfdaf8b156>:64} INFO - \n",
      "Executing query: \n",
      "\n",
      "    create table  if not exists sample.tb_test_num_tmp1 as\n",
      "    select t.f22, f02, min(f_date) as f_date, max(l_date) as l_date,f06,f07\n",
      "     from\n",
      "    ( select * from tb_test_num_tmp\n",
      "    union all\n",
      "    select * from tb_test_num ) t\n",
      "    group by t.f22,f02,f06,f07\n",
      ";\n",
      "[2023-05-19 12:17:26,614] root {<ipython-input-9-71dfdaf8b156>:68} INFO - Finished Query with 2 seconds\n",
      "[2023-05-19 12:17:26,618] root {<ipython-input-9-71dfdaf8b156>:71} INFO - \n",
      "Executing query: \n",
      "drop table if exists sample.tb_test_num;\n",
      "[2023-05-19 12:17:27,071] root {<ipython-input-9-71dfdaf8b156>:75} INFO - Finished Query with 0 seconds\n",
      "[2023-05-19 12:17:27,074] root {<ipython-input-9-71dfdaf8b156>:78} INFO - \n",
      "Executing query: \n",
      " alter table sample.tb_test_num_tmp1 rename to sample.tb_test_num; \n",
      "[2023-05-19 12:17:27,522] root {<ipython-input-9-71dfdaf8b156>:82} INFO - Finished Query with 0 seconds\n",
      "[2023-05-19 12:17:27,525] root {<ipython-input-9-71dfdaf8b156>:85} INFO - \n",
      "Executing query: \n",
      " select * from sample.tb_test_num; \n",
      "+---+-----------+-------------+-------------+---+---+\n",
      "|f22|f02        |f_date       |l_date       |f06|f07|\n",
      "+---+-----------+-------------+-------------+---+---+\n",
      "|3  |19218983880|2023-05-12-20|2023-05-17-08|0  |7  |\n",
      "|3  |19218983881|2023-05-12-20|2023-05-17-09|0  |7  |\n",
      "|3  |19218983882|2023-05-12-20|2023-05-17-10|0  |7  |\n",
      "|3  |19218983883|2023-05-12-21|2023-05-17-11|0  |7  |\n",
      "|3  |19218983884|2023-05-12-22|2023-05-17-11|0  |7  |\n",
      "|3  |19218983885|2023-05-12-20|2023-05-17-11|0  |7  |\n",
      "|3  |19218983886|2023-05-12-22|2023-05-17-11|0  |7  |\n",
      "|3  |19218983887|2023-05-12-20|2023-05-17-10|0  |7  |\n",
      "|3  |19218983888|2023-05-12-22|2023-05-17-11|0  |7  |\n",
      "|3  |19218983889|2023-05-12-21|2023-05-17-11|0  |7  |\n",
      "+---+-----------+-------------+-------------+---+---+\n",
      "\n",
      "[2023-05-19 12:17:27,902] root {<ipython-input-9-71dfdaf8b156>:90} INFO - Finished Query with 0 seconds\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"use sample;\") \n",
    "df = sqlContext.sql(\"show tables;\") \n",
    "df.show(100,False)\n",
    "\n",
    "query = \"drop table if exists sample.tb_test_num_tmp;\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "sqlContext.sql(query) \n",
    "\n",
    "query = \"\"\"\n",
    "    create table  if not exists sample.tb_test_num_tmp as\n",
    "    select f22, f02, min(bd) as f_date, max(bd) as l_date,f06,f07\n",
    "    from tb_test_qf_tmp1 \n",
    "    group by f22, f02,f06,f07;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"drop table if exists sample.tb_test_num_tmp1;\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"\"\"\n",
    "    create table if not exists sample.tb_test_num\n",
    "    as\n",
    "    select t.f22, f02, min(f_date) as f_date, max(l_date) as l_date,f06,f07\n",
    "    from sample.tb_test_num_tmp as t\n",
    "    group by t.f22,f02,f06,f07\n",
    "    limit 1;\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"Truncate table sample.tb_test_num;\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    create table  if not exists sample.tb_test_num_tmp1 as\n",
    "    select t.f22, f02, min(f_date) as f_date, max(l_date) as l_date,f06,f07\n",
    "     from\n",
    "    ( select * from tb_test_num_tmp\n",
    "    union all\n",
    "    select * from tb_test_num ) t\n",
    "    group by t.f22,f02,f06,f07\n",
    ";\"\"\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"drop table if exists sample.tb_test_num;\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \" alter table sample.tb_test_num_tmp1 rename to sample.tb_test_num; \"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \" select * from sample.tb_test_num; \"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "df = sqlContext.sql(query) \n",
    "df.show(100, False)\n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
