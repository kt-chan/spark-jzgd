{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --upgrade pip\n",
    "!pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,logging\n",
    "import argparse\n",
    "\n",
    "date_strftime_format = \"%d-%b-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='[%(asctime)s] %(name)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--f\", \"--filepath\", type=str, default=\"./conf/default.conf\", help='provide configuration filepath')\n",
    "args = parser.parse_args(args=['--filepath', './conf/default.conf'])\n",
    "configFilePath = args.f\n",
    "\n",
    "from configparser import ConfigParser\n",
    "config_object = ConfigParser()\n",
    "config_object.read(configFilePath)\n",
    "scaleinfo = config_object[\"SCALEINFO\"]\n",
    "scale = scaleinfo.getint(\"scale_gb\")\n",
    "batch = scaleinfo.getint(\"batch_k\")\n",
    "timespan_days = scaleinfo.getint(\"timespan_days\")\n",
    "droptable = scaleinfo.getboolean(\"droptable\")\n",
    "debugMode = scaleinfo.getboolean(\"debugMode\")\n",
    "\n",
    "logging.info(\"Using following scale configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SCALEINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)\n",
    "\n",
    "    \n",
    "systeminfo = config_object[\"SYSTEMINFO\"]\n",
    "SPARK_APP_NAME = str(systeminfo.get(\"spark.app.name\")).strip('\\\"')\n",
    "SPARK_MASTER = str(systeminfo.get(\"spark.master.hostpath\")).strip('\\\"')\n",
    "HIVE_HMS_HOST= str(systeminfo.get(\"hive.metastore.uris\")).strip('\\\"')\n",
    "SPARK_WAREHOUSE_DIR = str(systeminfo.get(\"spark.sql.warehouse.dir\")).strip('\\\"')\n",
    "SPARK_DRIVER_CORES = systeminfo.getint(\"spark_driver_cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.driver.memory\")).strip('\\\"')\n",
    "SPARK_EXECUTOR_CORES = systeminfo.getint(\"spark.executor.cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.executor.memory\")).strip('\\\"')\n",
    "\n",
    "logging.info(\"Using following system configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SYSTEMINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(SPARK_APP_NAME) \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"hive.metastore.uris\", HIVE_HMS_HOST) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", SPARK_WAREHOUSE_DIR) \\\n",
    "        .config(\"spark_driver_cores\", SPARK_DRIVER_CORES) \\\n",
    "        .config(\"spark.driver.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .config(\"spark.executor.cores\", SPARK_EXECUTOR_CORES) \\\n",
    "        .config(\"spark.executor.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "sqlContext = SQLContext(spark.sparkContext, sparkSession=spark)\n",
    "spark.sparkContext.version\n",
    "logging.info(\"Spark Version: \" + spark.version)\n",
    "logging.info(\"PySpark Version: \" + pyspark.__version__)\n",
    "logging.info(\"Pandas Version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from time import sleep\n",
    "\n",
    "df = pd.DataFrame()   \n",
    "taskdone = False\n",
    "alphabet = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "LENGTH = 10\n",
    "\n",
    "def getNumberOfExecutor():\n",
    "    number_of_workers = len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "    return number_of_workers\n",
    "\n",
    "def progressbar():\n",
    "    logging.info('Start')\n",
    "    while not (taskdone):\n",
    "        sleep(5)\n",
    "        logging.info('.')\n",
    "\n",
    "def inittask(droptable = False):\n",
    "    sqlContext.sql(\"create database if not exists sample;\")\n",
    "    sqlContext.sql(\"use sample;\")\n",
    "    \n",
    "    if(droptable):\n",
    "        logging.info('dropping sample.* tables')\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_sev_u;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test_qf_stat;\")\n",
    "    \n",
    "\n",
    "def gendata_tbsevu(scale = 1, batch_size = 100000):\n",
    "    logging.info(\"generating data for tb_sev_u ...\")\n",
    "    \n",
    "    sqlContext.sql(\"\"\"\n",
    "        create table sample.tb_sev_u \n",
    "        as\n",
    "        select distinct f02 as id, f03 as name\n",
    "        from sample.tb_test \n",
    "        where f02 < 19218983880 or f02 > 19218983890\n",
    "        ORDER BY RAND () \n",
    "        limit {batch}\n",
    "    ;\"\"\".format(batch=batch_size) )\n",
    "\n",
    "    logging.info(\"finished loading data for tb_sev_u with \" + str(batch_size) + \" rows.\")    \n",
    "    \n",
    "    \n",
    "def gendata_tbtest_noise(scale_rounds = 1, batch_size = 100000, timespan_days = 31):\n",
    "    \n",
    "    logging.info(\"generating noise data for tb_test ...\")\n",
    "    ## for 100,000 rows of data with 25MB Storage Data Without Replication\n",
    "    global df\n",
    "    target_ts_int=None\n",
    "    partitions = timespan_days * 24\n",
    "    \n",
    "    for i in range(scale_rounds):\n",
    "\n",
    "        if 'df' in globals():\n",
    "            del df\n",
    "        \n",
    "        df = pd.DataFrame()    \n",
    "        \n",
    "        logging.info(\"running round: \" + str(i+1) + \" of total \" + str(scale_rounds)  +\" rounds, with batchsize = \" + str(batch_size) + \".\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        cols=[\"f01\", \"f02\",\"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\"f18\",\"f19\",\"f20\",\"f21\",\"f22\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\",\"f30\",\"f31\" ]\n",
    "        random_txtcols=[\"f03\",\"f05\",\"f08\",\"f09\",\"f11\",\"f12\",\"f13\",\"f16\",\"f17\",\"f18\",\"f20\",\"f21\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\"]\n",
    "\n",
    "        current_ts = pd.Timestamp.now()\n",
    "        current_ts_int = int(pd.to_datetime(current_ts).value/10**9)\n",
    "        if target_ts_int is None:\n",
    "            initial_ts = pd.to_datetime(current_ts - pd.Timedelta(days=timespan_days))\n",
    "        else:\n",
    "            initial_ts = target_ts;\n",
    "        \n",
    "        target_ts =  pd.to_datetime(initial_ts + pd.Timedelta(hours=math.ceil(partitions / scale_rounds)))\n",
    "        initial_ts_int =  int(initial_ts.value/10**9)\n",
    "        target_ts_int =  int(target_ts.value/10**9)\n",
    "        \n",
    "        df1 = pd.DataFrame(np.random.randint(initial_ts_int, target_ts_int, size=(batch_size,1)),  columns=list([\"f01\"]))\n",
    "        df2 = pd.DataFrame(np.random.randint(19200000000,19200000000+20000000,size=(batch_size,2)), columns=list([\"f02\", \"f04\"])) \n",
    "        df6 = pd.DataFrame(np.random.randint(0, 10, size=(batch_size,5)),  columns=list([\"f06\", \"f07\", \"f10\", \"f15\", \"f19\"]))\n",
    "        df14 = pd.DataFrame(np.random.randint(48, 51, size=(batch_size,1)),  columns=list([\"f14\"]))\n",
    "        df22 = pd.DataFrame(np.random.randint(1, 10, size=(batch_size,1)),  columns=list([\"f22\"]))\n",
    "        df30 = pd.DataFrame(initial_ts_int, index=range(batch_size),  columns=list([\"f30\"]))\n",
    "        df31 = pd.DataFrame(target_ts_int, index=range(batch_size),  columns=list([\"f31\"]))\n",
    "\n",
    "        for k in random_txtcols: \n",
    "            np_batchsize = None\n",
    "            if k == 'f16' :\n",
    "                np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, math.ceil(1 + LENGTH * (np.random.randint(1, 30) / 10))])\n",
    "            else :\n",
    "                np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, LENGTH])\n",
    "                \n",
    "            df0 = pd.DataFrame( [\"\".join(np_batchsize[i]) for i in range(len(np_batchsize))], columns=[k])\n",
    "            df[k] = df0[k]\n",
    "\n",
    "        df = pd.concat([df1,df2, df6, df14, df22, df30, df31, df], axis=1, join='inner')\n",
    "\n",
    "        df = df[cols]\n",
    "        logging.info(\"Memory Usage: \" + f'{df.memory_usage(deep=True).sum():,}'  + \" Bytes\")\n",
    "        \n",
    "        logging.info(\"creating spark data frame for partitioins from \" + str(pd.to_datetime(df['f01'].min(), unit='s')) + ' to ' +  str(pd.to_datetime(df['f01'].max(), unit='s')) + '... ')\n",
    "        sparkDF = spark.createDataFrame(df)\n",
    "        sparkDF = sparkDF.withColumn(\"cp\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"ld\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"f30\", from_unixtime(sparkDF[\"f30\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"f31\", from_unixtime(sparkDF[\"f31\"], \"yyyyMMddHH\"))\n",
    "        #sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).bucketBy(32, \"f02\").sortBy(\"f01\").format(\"orc\").saveAsTable(\"tb_test\")\n",
    "        logging.info(\"writing to hive table sample.tb_test ...\")\n",
    "        sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).format(\"orc\").saveAsTable(\"tb_test\")\n",
    "\n",
    "        finish_ts = pd.Timestamp.now()\n",
    "        spark.catalog.clearCache()\n",
    "        logging.info(\"finished round: \"+ str(i+1) + \" for \" + str(batch_size) + \" rows with \" + str( (finish_ts - current_ts).seconds)  + \" seconds\")\n",
    "\n",
    "def gendata_tbtest_target(scale_rounds = 1, batch_size = 100000, timespan_days = 31):\n",
    "    \n",
    "    logging.info(\"generating target data for tb_test ...\")\n",
    "    ## for 100,000 rows of data with 25MB Storage Data Without Replication\n",
    "    global df\n",
    "    target_ts_int=None\n",
    "    partitions = timespan_days * 24\n",
    "    \n",
    "    for i in range(scale_rounds):\n",
    "        if 'df' in globals():\n",
    "            del df\n",
    "\n",
    "        df = pd.DataFrame()   \n",
    "\n",
    "        cols=[\"f01\", \"f02\",\"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\"f18\",\"f19\",\"f20\",\"f21\",\"f22\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\",\"f30\",\"f31\" ]\n",
    "        random_txtcols=[\"f03\",\"f05\",\"f08\",\"f09\",\"f11\",\"f12\",\"f13\",\"f16\",\"f17\",\"f18\",\"f20\",\"f21\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\"]\n",
    "\n",
    "        current_ts = pd.Timestamp.now()\n",
    "        current_ts_int = int(pd.to_datetime(current_ts).value/10**9)\n",
    "        if target_ts_int is None:\n",
    "            initial_ts = pd.to_datetime(current_ts - pd.Timedelta(days=timespan_days))\n",
    "        else:\n",
    "            initial_ts = target_ts;\n",
    "\n",
    "        target_ts =  pd.to_datetime(initial_ts + pd.Timedelta(hours=math.ceil(partitions / scale_rounds)))\n",
    "        initial_ts_int =  int(initial_ts.value/10**9)\n",
    "        target_ts_int =  int(target_ts.value/10**9)\n",
    "\n",
    "        ## /*\n",
    "        ##\n",
    "        ## +---+-----------+------------------+---+---+---+----------+----------+\n",
    "        ## |f22|f02        |f16               |cnt|f06|f07|bd        |ad        |\n",
    "        ## +---+-----------+------------------+---+---+---+----------+----------+\n",
    "        ## |3  |19218983709|xfhtE3h5CYgGOWhOKk|1  |0  |7  |2023-03-19|2023031415|\n",
    "        ## |5  |19219966684|ygeEVbANtFFi8Y1hVk|1  |4  |4  |2023-03-19|2023031415|\n",
    "        ## |3  |19218120415|rUpWrl5GvPKANvoVQA|1  |6  |2  |2023-03-19|2023031415|\n",
    "        ## |8  |19204926044|giFVrAhopgs0xLBbqC|1  |9  |4  |2023-03-19|2023031415|\n",
    "        ## |3  |19200131747|WXCnls2FOur2a2eDrx|1  |4  |7  |2023-03-19|2023031415|\n",
    "        ## |9  |19213837980|Uvsg7bYQhZAddPp4fb|1  |0  |7  |2023-03-19|2023031415|\n",
    "        ## |8  |19206179720|7KZIluL2WBkkQHI6M4|1  |5  |0  |2023-03-19|2023031415|\n",
    "        ## |8  |19204624908|MNwN3ensBpDhy08k18|1  |1  |3  |2023-03-19|2023031415|\n",
    "        ## |1  |19219995818|GVkuR9l4m3ZQQO9duG|1  |5  |4  |2023-03-19|2023031415|\n",
    "        ## |4  |19219858050|1U2HdpMfVarddjhWNl|1  |0  |6  |2023-03-19|2023031415|\n",
    "        ## +---+-----------+------------------+---+---+---+----------+----------+\n",
    "        ## \n",
    "        ## */\n",
    "\n",
    "        df1 = pd.DataFrame(np.random.randint(initial_ts_int, target_ts_int, size=(batch_size,1)),  columns=list([\"f01\"]))\n",
    "        df2 = pd.DataFrame(np.random.randint(19218983880,19218983880+10,size=(batch_size,1)), columns=list([\"f02\"])) \n",
    "        df4 = pd.DataFrame(np.random.randint(19200000000,19200000000+20000000,size=(batch_size,1)), columns=list([\"f04\"])) \n",
    "        df6 = pd.DataFrame(0, index=range(batch_size),  columns=list([\"f06\"]))\n",
    "        df7 = pd.DataFrame(7, index=range(batch_size),  columns=list([\"f07\"]))\n",
    "        df10 = pd.DataFrame(np.random.randint(0, 10, size=(batch_size,3)),  columns=list([\"f10\", \"f15\", \"f19\"]))\n",
    "        df14 = pd.DataFrame(np.random.randint(48, 51, size=(batch_size,1)),  columns=list([\"f14\"]))\n",
    "        df22 = pd.DataFrame(3, index=range(batch_size),  columns=list([\"f22\"]))\n",
    "        df30 = pd.DataFrame(np.random.randint(initial_ts_int, target_ts_int, size=(batch_size,2)),  columns=list([\"f30\", \"f31\"]))\n",
    "\n",
    "        for k in random_txtcols: \n",
    "            if k == 'f16' :\n",
    "                np_batchsize =  pd.DataFrame(\"Great you found me !\", index=range(batch_size),  columns=list([\"f16\"]))\n",
    "            else :\n",
    "                np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, LENGTH])\n",
    "                np_batchsize = pd.DataFrame( [\"\".join(np_batchsize[i]) for i in range(len(np_batchsize))], columns=[k])\n",
    "\n",
    "            df[k] = np_batchsize\n",
    "\n",
    "        df = pd.concat([df1,df2, df4, df6, df7, df10,df14, df22, df30, df], axis=1, join='inner')\n",
    "\n",
    "        df = df[cols]\n",
    "        \n",
    "        logging.info(\"Memory Usage: \" + f'{df.memory_usage(deep=True).sum():,}'  + \" Bytes\")\n",
    "\n",
    "        logging.info(\"creating spark data frame for partitioins from \" + str(pd.to_datetime(df['f01'].min(), unit='s')) + ' to ' +  str(pd.to_datetime(df['f01'].max(), unit='s')) + '... ')\n",
    "        sparkDF = spark.createDataFrame(df)\n",
    "        sparkDF = sparkDF.withColumn(\"cp\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"ld\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"f30\", from_unixtime(sparkDF[\"f30\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"f31\", from_unixtime(sparkDF[\"f31\"], \"yyyyMMddHH\"))\n",
    "        #sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).bucketBy(32, \"f02\").sortBy(\"f01\").format(\"orc\").saveAsTable(\"tb_test\")\n",
    "        logging.info(\"writing to hive table sample.tb_test ...\")\n",
    "        sparkDF.coalesce(getNumberOfExecutor()).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).format(\"orc\").saveAsTable(\"tb_test\")\n",
    "\n",
    "        finish_ts = pd.Timestamp.now()\n",
    "        spark.catalog.clearCache()\n",
    "        logging.info(\"finished \" + str(batch_size) + \" rows with \" + str( (finish_ts - current_ts).seconds)  + \" seconds\")\n",
    "\n",
    "    \n",
    "def gendata_tbtest(scale = 1, batch_size = 100000, timespan_days = 31):\n",
    "    \n",
    "    scale_factor = scale*1024\n",
    "    scale_unit =  math.ceil(batch_size/(100000/25))\n",
    "    scale_rounds = math.ceil(scale_factor/scale_unit)\n",
    "    \n",
    "    \n",
    "    #Testing\n",
    "    if debugMode :\n",
    "        logging.info(\"Testing with \" + str(1) +\" batches data instead of \" + str(scale_rounds) +\" ...., remove this block later.\")\n",
    "        scale_rounds = 1    \n",
    "            \n",
    "    gendata_tbtest_noise(scale_rounds, batch_size, timespan_days)\n",
    "    gendata_tbtest_target(scale_rounds, math.ceil(batch_size/10), timespan_days)\n",
    "    \n",
    "    sqlContext.sql(\"use sample;\")\n",
    "    sqlContext.sql(\"show tables\").show()\n",
    "\n",
    "def gendata(scale_gb = 1, batch_size_k = 100, timespan_days = 31):\n",
    "    logging.info(\"number_of_workers: \" + str(getNumberOfExecutor()) + \".\")\n",
    "    gendata_tbtest(scale_gb, batch_size_k * 1000, timespan_days )\n",
    "    gendata_tbsevu(scale_gb, batch_size_k * 1000)\n",
    "    \n",
    "    \n",
    "def longtask(scale_gb = 1, batch_size_k = 100, timespan_days = 31, droptTable = False):\n",
    "    inittask(droptTable)\n",
    "    gendata(scale_gb, batch_size_k, timespan_days)\n",
    "    global taskdone\n",
    "    taskdone = True\n",
    "\n",
    "# start the thread pool\n",
    "t1 = threading.Thread(target=progressbar)\n",
    "t2 = threading.Thread(target=longtask,  args=(scale, batch, timespan_days, droptable))\n",
    "\n",
    "# wait for all tasks to complete\n",
    "# start threads\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until threads finish their job\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS  ;\")\n",
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS  ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_test;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"ANALYZE TABLE sample.tb_sev_u COMPUTE STATISTICS FOR ALL COLUMNS  ;\")\n",
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_sev_u COMPUTE STATISTICS FOR ALL COLUMNS  ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_sev_u;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
