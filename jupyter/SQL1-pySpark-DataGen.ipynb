{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !pip install --upgrade pip\n",
    "## !pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.3.1\n",
      "PySpark Version: 3.3.1\n",
      "Pandas Version: 1.3.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "\n",
    "SPARK_WAREHOUSE_DIR = \"hdfs://hadoop-service:9000/user/hive/warehouse\"\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Remote Spark\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hms-service:9083\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", SPARK_WAREHOUSE_DIR) \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1G\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext, sparkSession=spark)\n",
    "\n",
    "print(\"Spark Version: \" + spark.version)\n",
    "print(\"PySpark Version: \" + pyspark.__version__)\n",
    "print(\"Pandas Version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##   sqlContext.sql(\"\"\"\n",
    "##       CREATE TABLE if not exists sample.tb_test\n",
    "##       (\n",
    "##       f01    BIGINT comment 'unix_timestamp',\n",
    "##       f02    string comment 'f02本端号码（去重后2千万）',\n",
    "##       f03    string,\n",
    "##       f04    string comment 'f04对端号码（去重后2千万）, count distinct clause',\n",
    "##       f05    string,\n",
    "##       f06    string comment 'group by clause, ref-a',\n",
    "##       f07    string comment 'group by clause, ref-b',\n",
    "##       f08    string,\n",
    "##       f09    string,\n",
    "##       f10    INT,\n",
    "##       f11    string,\n",
    "##       f12    string,\n",
    "##       f13    string,\n",
    "##       f14    string comment 'data filtering col, where f14 = 49',\n",
    "##       f15    BIGINT,\n",
    "##       f16    string comment '中文文本内容，需要制表符转变',\n",
    "##       f17    string,\n",
    "##       f18    string,\n",
    "##       f19    INT,\n",
    "##       f20    string,\n",
    "##       f21    string,\n",
    "##       f22    INT comment '1st group by clause, maybe category',\n",
    "##       f23    string,\n",
    "##       f24    string,\n",
    "##       f25    string,\n",
    "##       f26    string,\n",
    "##       f27    string,\n",
    "##       f28    string,\n",
    "##       f29    string,\n",
    "##       f30    bigint comment 'business datetime',\n",
    "##       f31    bigint comment 'processing datetime'\n",
    "##       ) partitioned by (cp bigint, ld bigint) \n",
    "##       stored as orc tblproperties (\"orc.compress\"=\"ZLIB\");\n",
    "##   \"\"\")\n",
    "\n",
    "##   sqlContext.sql(\"\"\"\n",
    "##       create table if not exists sample.tb_sev_u (id string, name string)\n",
    "##       stored as orc tblproperties (\"orc.compress\"=\"ZLIB\");\n",
    "##   \"\"\")\n",
    "\n",
    "##   sqlContext.sql(\"\"\"\n",
    "##       create table if not exists sample.tb_test_qf_stat(\n",
    "##       f22 string,\n",
    "##       f02 string,\n",
    "##       f16 string,\n",
    "##       cnt bigint,\n",
    "##       f06    string,\n",
    "##       f07    string\n",
    "##       ) partitioned by (bd string, ad bigint)\n",
    "##       stored as orc tblproperties (\"orc.compress\"=\"ZLIB\");\n",
    "##   \"\"\")\n",
    "##   \n",
    "##   df = sqlContext.sql(\"\"\"show tables;\"\"\")\n",
    "##   print('\\n')\n",
    "##   df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "................"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from time import sleep\n",
    "\n",
    "number_of_workers = 0\n",
    "taskdone = False\n",
    "alphabet = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "LENGTH = 10\n",
    "\n",
    "def getNumberOfExecutor():\n",
    "    number_of_workers = len(spark._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "\n",
    "def progressbar():\n",
    "    print('Start')\n",
    "    while not (taskdone):\n",
    "        sleep(5)\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "def inittask(droptable = False):\n",
    "    sqlContext.sql(\"create database if not exists sample;\")\n",
    "    sqlContext.sql(\"use sample;\")\n",
    "    \n",
    "    if(droptable):\n",
    "        print('dropping sample.* tables')\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_sev_u;\")\n",
    "        sqlContext.sql(\"drop table if exists sample.tb_test_qf_stat;\")\n",
    "\n",
    "    \n",
    "def gendata_tbsevu(scale = 1, batch_size = 10000):\n",
    "    print(\"\\ngenerating data for tb_sev_u ...\")\n",
    "    \n",
    "    cols=[\"id\", \"name\"]\n",
    "    for i in range(scale):\n",
    "        \n",
    "        if 'df' in globals():\n",
    "            del df\n",
    "        \n",
    "        df = pd.DataFrame()    \n",
    "        print(\"running round: \" + str(i+1) + \" of total \" + str(scale)  +\" rounds, with batchsize = \" + str(batch_size) + \".\")\n",
    "        start_ts = pd.Timestamp.now()\n",
    "        dfid = pd.DataFrame(np.random.randint(19200000000,19200000000+20000000,size=(batch_size,1)), columns=list([\"id\"])) \n",
    "\n",
    "        random_txtcols=[\"name\"]\n",
    "        for k in random_txtcols: \n",
    "            np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, LENGTH])\n",
    "            df0 = pd.DataFrame( [\"\".join(np_batchsize[i]) for i in range(len(np_batchsize))], columns=[k])\n",
    "            df[k] = df0[k]\n",
    "        \n",
    "        df = pd.concat([dfid, df], axis=1, join='inner')\n",
    "        df = df[cols]\n",
    "\n",
    "        sparkDF = spark.createDataFrame(df)    \n",
    "        sparkDF = sparkDF.sort([\"id\"])\n",
    "        sparkDF.coalesce(number_of_workers).write.mode(\"append\").format(\"hive\").saveAsTable(\"tb_sev_u\")\n",
    "        finish_ts = pd.Timestamp.now()\n",
    "        print(\"\\nfinished round: \"+ str(i+1) + \" for \" + str(batch_size) + \" rows with \" + str( (finish_ts - start_ts).seconds)  + \" seconds\")\n",
    "    \n",
    "\n",
    "    \n",
    "def gendata_tbtest(scale = 1, batch_size = 10000):\n",
    "    \n",
    "    print(\"\\ngenerating data for tb_test ...\")\n",
    "    \n",
    "    scale_factor = scale*1000\n",
    "    scale_unit =  math.ceil(10000/300)\n",
    "    scale_rounds = math.ceil(scale_factor/scale_unit)\n",
    "    \n",
    "    for i in range(scale_rounds):\n",
    "        if 'df' in globals():\n",
    "            del df\n",
    "\n",
    "        print(\"running round: \" + str(i+1) + \" of total \" + str(scale_rounds)  +\" rounds, with batchsize = \" + str(batch_size) + \".\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        cols=[\"f01\", \"f02\",\"f03\", \"f04\", \"f05\", \"f06\", \"f07\", \"f08\", \"f09\", \"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\"f18\",\"f19\",\"f20\",\"f21\",\"f22\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\",\"f30\",\"f31\" ]\n",
    "        random_txtcols=[\"f03\",\"f05\",\"f08\",\"f09\",\"f11\",\"f12\",\"f13\",\"f16\",\"f17\",\"f18\",\"f20\",\"f21\",\"f23\",\"f24\",\"f25\",\"f26\",\"f27\",\"f28\",\"f29\"]\n",
    "\n",
    "        start_ts = pd.Timestamp.now()\n",
    "        start_ts_int = int(pd.to_datetime(start_ts).value/10**9)\n",
    "        initial_ts_int = int(pd.to_datetime(start_ts - pd.Timedelta(days=60)).value/10**9)\n",
    "        df2 = pd.DataFrame(np.random.randint(19200000000,19200000000+20000000,size=(batch_size,2)), columns=list([\"f02\", \"f04\"])) \n",
    "        df1 = pd.DataFrame(np.random.randint(initial_ts_int, start_ts_int, size=(batch_size,1)),  columns=list([\"f01\"]))\n",
    "        df6 = pd.DataFrame(np.random.randint(0, 10, size=(batch_size,5)),  columns=list([\"f06\", \"f07\", \"f10\", \"f15\", \"f19\"]))\n",
    "        df14 = pd.DataFrame(np.random.randint(48, 51, size=(batch_size,1)),  columns=list([\"f14\"]))\n",
    "        df22 = pd.DataFrame(np.random.randint(1, 10, size=(batch_size,1)),  columns=list([\"f22\"]))\n",
    "        df30 = pd.DataFrame(np.random.randint(start_ts_int-(30*24*3600), start_ts_int, size=(batch_size,2)),  columns=list([\"f30\",\"f31\"]))\n",
    "\n",
    "        for k in random_txtcols: \n",
    "            np_batchsize = np.random.choice(np.array(alphabet, dtype=\"|U1\"), [batch_size, LENGTH])\n",
    "            df0 = pd.DataFrame( [\"\".join(np_batchsize[i]) for i in range(len(np_batchsize))], columns=[k])\n",
    "            df[k] = df0[k]\n",
    "\n",
    "        df = pd.concat([df1,df2, df6, df14, df22, df30, df], axis=1, join='inner')\n",
    "\n",
    "        df = df[cols]\n",
    "\n",
    "\n",
    "        sparkDF = spark.createDataFrame(df)\n",
    "        sparkDF = sparkDF.withColumn(\"cp\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\"))\n",
    "        sparkDF = sparkDF.withColumn(\"ld\", from_unixtime(sparkDF[\"f01\"], \"yyyyMMddHH\"))\n",
    "        sparkDF.coalesce(number_of_workers).write.mode(\"append\").partitionBy( [\"cp\",\"ld\"]).bucketBy(32, \"f02\").sortBy(\"f01\").format(\"orc\").saveAsTable(\"tb_test\")\n",
    "\n",
    "        finish_ts = pd.Timestamp.now()\n",
    "        print(\"\\nfinished round: \"+ str(i+1) + \" for \" + str(batch_size) + \" rows with \" + str( (finish_ts - start_ts).seconds)  + \" seconds\")\n",
    "\n",
    "    sqlContext.sql(\"use sample;\")\n",
    "    sqlContext.sql(\"show tables\").show()\n",
    "    \n",
    "def gendata(scale = 1, batch_size = 10000):\n",
    "    print(\"number_of_workers: \" + str(getNumberOfExecutor()) + \".\")\n",
    "    gendata_tbsevu(scale, batch_size)\n",
    "    gendata_tbtest(scale, batch_size)\n",
    "    \n",
    "    \n",
    "def longtask(scale = 1, batch_size = 10000):\n",
    "    inittask(False)\n",
    "    gendata(scale, batch_size)\n",
    "    global taskdone\n",
    "    taskdone = True\n",
    "\n",
    "# start the thread pool\n",
    "t1 = threading.Thread(target=progressbar)\n",
    "t2 = threading.Thread(target=longtask,  args=(1, 1000))\n",
    "\n",
    "# wait for all tasks to complete\n",
    "# start threads\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# wait until threads finish their job\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "print('\\nDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sqlContext.sql(\"use sample;\")\n",
    "df = sqlContext.sql(\" select count(*) from tb_test;\")\n",
    "df.show()\n",
    "\n",
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS \")\n",
    "\n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_test \")\n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"use sample;\") \n",
    "df = sqlContext.sql(\"show tables\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_sev_u COMPUTE STATISTICS FOR ALL COLUMNS ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_sev_u;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_test COMPUTE STATISTICS FOR ALL COLUMNS ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_test;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
