{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install SQLAlchemy==1.3\n",
    "!pip install pyhive[hive]\n",
    "!pip install thrift\n",
    "!pip install sasl\n",
    "!pip install thrift-sasl \n",
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/**\n",
    "注意：\n",
    "@date1表示开始时间，@date2表示结束时间，一般两者相差一天，即取一天的数据。\n",
    "bd/ad/ld/cp格式都是yyyymmddhh24，比如2023032301\n",
    "@batchId表示批次的变量，yyyymmddhh24格式，比如2023032301\n",
    "udf_is_null 自定义函数，用于判断字段是否为空，如果字段为空/null或者空字符串，都返回0，否则返回1\n",
    "--主表建表语句 TABLE tb_test --每天1T数据，f02本端号码（去重后2千万），f04对端号码（去重后2千万），f16中文内容\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys,os,logging\n",
    "import argparse\n",
    "\n",
    "date_strftime_format = \"%d-%b-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='[%(asctime)s] %(name)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--f\", \"--filepath\", type=str, default=\"./conf/default.conf\", help='provide configuration filepath')\n",
    "args = parser.parse_args(args=['--filepath', './conf/default.conf'])\n",
    "configFilePath = args.f\n",
    "\n",
    "from configparser import ConfigParser\n",
    "config_object = ConfigParser()\n",
    "config_object.read(configFilePath)\n",
    "scaleinfo = config_object[\"SCALEINFO\"]\n",
    "scale = scaleinfo.getint(\"scale_gb\")\n",
    "batch = scaleinfo.getint(\"batch_k\")\n",
    "timespan_days = scaleinfo.getint(\"timespan_days\")\n",
    "droptable = scaleinfo.getboolean(\"droptable\")\n",
    "\n",
    "logging.info(\"Using following scale configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SCALEINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)\n",
    "\n",
    "    \n",
    "systeminfo = config_object[\"SYSTEMINFO\"]\n",
    "SPARK_APP_NAME = str(systeminfo.get(\"spark.app.name\")).strip('\\\"')\n",
    "SPARK_MASTER = str(systeminfo.get(\"spark.master.hostpath\")).strip('\\\"')\n",
    "HIVE_HMS_HOST= str(systeminfo.get(\"hive.metastore.uris\")).strip('\\\"')\n",
    "SPARK_WAREHOUSE_DIR = str(systeminfo.get(\"spark.sql.warehouse.dir\")).strip('\\\"')\n",
    "SPARK_DRIVER_CORES = systeminfo.getint(\"spark_driver_cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.driver.memory\")).strip('\\\"')\n",
    "SPARK_EXECUTOR_CORES = systeminfo.getint(\"spark.executor.cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.executor.memory\")).strip('\\\"')\n",
    "\n",
    "logging.info(\"Using following system configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SYSTEMINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "from time import sleep\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(SPARK_APP_NAME) \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"hive.metastore.uris\", HIVE_HMS_HOST) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", SPARK_WAREHOUSE_DIR) \\\n",
    "        .config(\"spark_driver_cores\", SPARK_DRIVER_CORES) \\\n",
    "        .config(\"spark.driver.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .config(\"spark.executor.cores\", SPARK_EXECUTOR_CORES) \\\n",
    "        .config(\"spark.executor.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "sqlContext = SQLContext(spark.sparkContext, sparkSession=spark)\n",
    "spark.sparkContext.version\n",
    "logging.info(\"Spark Version: \" + spark.version)\n",
    "logging.info(\"PySpark Version: \" + pyspark.__version__)\n",
    "logging.info(\"Pandas Version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_table_and_view():\n",
    "    \n",
    "    logging.info(\"Executing query: drop tables and views ...\" )\n",
    "    sqlContext.sql(\"use sample;\") \n",
    "    sqlContext.sql(\"drop table if exists sample.tb_test_qf_stat;\") \n",
    "    sqlContext.sql(\"drop view if exists tb_test_qf_tmp1;\") \n",
    "\n",
    "    \n",
    "def create_tb_test_qf_stat():\n",
    "    \n",
    "    sqlContext.sql(\"use sample;\") \n",
    "    stmt = \"\"\"\n",
    "            create table if not exists sample.tb_test_qf_stat(\n",
    "            f22 string,\n",
    "            f02 string,\n",
    "            f16 string,\n",
    "            cnt bigint,\n",
    "            f06    string,\n",
    "            f07    string\n",
    "            )partitioned by (bd string, ad bigint)\n",
    "            ;\n",
    "           \"\"\"\n",
    "    \n",
    "    logging.info(\"Executing query: \\n\" + stmt)\n",
    "    sqlContext.sql(stmt)\n",
    "\n",
    "    \n",
    "def create_view_tb_test_qf_tmp1():\n",
    "\n",
    "    sqlContext.sql(\"use sample;\") \n",
    "    query = \"\"\"\n",
    "        CREATE TEMPORARY VIEW tb_test_qf_tmp1 \n",
    "        as\n",
    "        with tb_test_qf_tmp as (\n",
    "            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n",
    "            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\\\?+','\\?') ,'0+','0'),'[ \\t]+',' ') as f16,\n",
    "            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd')  as bd,\n",
    "            {batchId} as ad,\n",
    "            trim(f06) as f06,trim(f07) as f07\n",
    "            from tb_test t\n",
    "            where f31 >= {date1} and f31 < {date2} and f14 = '49'\n",
    "        ),\n",
    "        tb_test_qf_tmp1 as (\n",
    "        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n",
    "        a.f06,a.f07\n",
    "        from tb_test_qf_tmp a\n",
    "        left join tb_sev_u b on a.f02 = b.id\n",
    "        where a.f02 is not null and b.id is null\n",
    "        and length(a.f16) > 10\n",
    "        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n",
    "        having cnt > 10\n",
    "        )\n",
    "        select * from tb_test_qf_tmp1\n",
    "        ;\n",
    "    \"\"\".format(batchId=batchId, date1=date1, date2=date2)\n",
    "    \n",
    "    logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "    runstarttime = datetime.now()\n",
    "    sqlContext.sql(query) \n",
    "    runfinishtime = datetime.now()\n",
    "    \n",
    "    logging.info(\"Finished Create view Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-17 18:49:57,949] root {<ipython-input-6-52af10efeb0a>:3} INFO - Executing query: drop tables and views ...\n",
      "[2023-05-17 18:49:58,823] root {<ipython-input-6-52af10efeb0a>:25} INFO - Executing query: \n",
      "\n",
      "            create table if not exists tb_test_qf_stat(\n",
      "            f22 string,\n",
      "            f02 string,\n",
      "            f16 string,\n",
      "            cnt bigint,\n",
      "            f06    string,\n",
      "            f07    string\n",
      "            )partitioned by (bd string, ad bigint)\n",
      "            ;\n",
      "           \n",
      "[2023-05-17 18:49:59,010] root {<ipython-input-6-52af10efeb0a>:58} INFO - \n",
      "Executing query: \n",
      "\n",
      "        CREATE TEMPORARY VIEW sample.tb_test_qf_tmp1 \n",
      "        as\n",
      "        with tb_test_qf_tmp as (\n",
      "            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n",
      "            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\?+','\\?') ,'0+','0'),'[ \t]+',' ') as f16,\n",
      "            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd')  as bd,\n",
      "            2023051018 as ad,\n",
      "            trim(f06) as f06,trim(f07) as f07\n",
      "            from tb_test t\n",
      "            where f31 >= 2023051018 and f31 < 2023051718 and f14 = '49'\n",
      "        ),\n",
      "        tb_test_qf_tmp1 as (\n",
      "        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n",
      "        a.f06,a.f07\n",
      "        from tb_test_qf_tmp a\n",
      "        left join tb_sev_u b on a.f02 = b.id\n",
      "        where a.f02 is not null and b.id is null\n",
      "        and length(a.f16) > 10\n",
      "        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n",
      "        having cnt > 10\n",
      "        )\n",
      "        select * from tb_test_qf_tmp1\n",
      "        ;\n",
      "    \n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\nIt is not allowed to add database prefix `sample` for the TEMPORARY view name.(line 2, pos 8)\n\n== SQL ==\n\n        CREATE TEMPORARY VIEW sample.tb_test_qf_tmp1 \n--------^^^\n        as\n        with tb_test_qf_tmp as (\n            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\?+','\\?') ,'0+','0'),'[ \t]+',' ') as f16,\n            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd')  as bd,\n            2023051018 as ad,\n            trim(f06) as f06,trim(f07) as f07\n            from tb_test t\n            where f31 >= 2023051018 and f31 < 2023051718 and f14 = '49'\n        ),\n        tb_test_qf_tmp1 as (\n        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n        a.f06,a.f07\n        from tb_test_qf_tmp a\n        left join tb_sev_u b on a.f02 = b.id\n        where a.f02 is not null and b.id is null\n        and length(a.f16) > 10\n        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n        having cnt > 10\n        )\n        select * from tb_test_qf_tmp1\n        ;\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b2cdcd9b1de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdrop_table_and_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcreate_tb_test_qf_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcreate_view_tb_test_qf_tmp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"show tables\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-52af10efeb0a>\u001b[0m in \u001b[0;36mcreate_view_tb_test_qf_tmp1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nExecuting query: \\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mrunstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mrunfinishtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \nIt is not allowed to add database prefix `sample` for the TEMPORARY view name.(line 2, pos 8)\n\n== SQL ==\n\n        CREATE TEMPORARY VIEW sample.tb_test_qf_tmp1 \n--------^^^\n        as\n        with tb_test_qf_tmp as (\n            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\?+','\\?') ,'0+','0'),'[ \t]+',' ') as f16,\n            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd')  as bd,\n            2023051018 as ad,\n            trim(f06) as f06,trim(f07) as f07\n            from tb_test t\n            where f31 >= 2023051018 and f31 < 2023051718 and f14 = '49'\n        ),\n        tb_test_qf_tmp1 as (\n        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n        a.f06,a.f07\n        from tb_test_qf_tmp a\n        left join tb_sev_u b on a.f02 = b.id\n        where a.f02 is not null and b.id is null\n        and length(a.f16) > 10\n        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n        having cnt > 10\n        )\n        select * from tb_test_qf_tmp1\n        ;\n    \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "today = datetime.now()\n",
    "target  = today - relativedelta(days=7)\n",
    "\n",
    "batchId = target.strftime(\"%Y%m%d%H\")\n",
    "date1 = target.strftime(\"%Y%m%d%H\")\n",
    "date2 = today.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "sqlContext.sql(\"use sample;\") \n",
    "sqlContext.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "\n",
    "drop_table_and_view()\n",
    "create_tb_test_qf_stat()\n",
    "create_view_tb_test_qf_tmp1()\n",
    "\n",
    "df = sqlContext.sql(\"show tables\") \n",
    "df.show()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "query = \"\"\"\n",
    " insert into tb_test_qf_stat partition(bd, ad)\n",
    " select f22,f02,f16,cnt,f06,f07,bd,ad\n",
    " from tb_test_qf_tmp1\n",
    ";\n",
    "\"\"\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished tb_test_qf_stat Insert Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"ANALYZE TABLE sample.tb_test_qf_stat  COMPUTE STATISTICS FOR ALL COLUMNS  ;\")\n",
    "sqlContext.sql(\"ANALYZE TABLE sample.tb_test_qf_stat  COMPUTE STATISTICS FOR ALL COLUMNS  ;\") \n",
    "df = sqlContext.sql(\"DESCRIBE EXTENDED sample.tb_test_qf_stat ;\") \n",
    "df.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sleep(5)\n",
    "\n",
    "sqlContext.sql(\"use sample;\") \n",
    "query = \"drop table if exists sample.tb_test_qf_lastest;\"\n",
    "sqlContext.sql(query) \n",
    "\n",
    "query = \"\"\"\n",
    "create table  if not exists tb_test_qf_lastest as\n",
    "select f22,f02,f16,cnt,bd,f06,f07 from\n",
    "( select *, row_number() over(partition by f02 order by bd desc ) rn from sample.tb_test_qf_stat) t\n",
    "where t.rn =1;\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(5)\n",
    "\n",
    "sqlContext.sql(\"use sample;\") \n",
    "query = \"drop table if exists sample.tb_test_num_tmp;\"\n",
    "sqlContext.sql(query) \n",
    "\n",
    "query = \"\"\"\n",
    "    create table  if not exists sample.tb_test_num_tmp as\n",
    "    select f22, f02, min(bd) as f_date, max(bd) as l_date,f06,f07\n",
    "    from tb_test_qf_tmp1 \n",
    "    group by f22, f02,f06,f07;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"use sample;\") \n",
    "\n",
    "query = \"drop table if exists sample.tb_test_num_tmp1;\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"\"\"\n",
    "    create table if not exists sample.tb_test_num\n",
    "    as\n",
    "    select t.f22, f02, min(f_date) as f_date, max(l_date) as l_date,f06,f07\n",
    "    from sample.tb_test_num_tmp as t\n",
    "    where 1 = 2\n",
    "    group by t.f22,f02,f06,f07\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"\"\"\n",
    "    create table  if not exists sample.tb_test_num_tmp1 as\n",
    "    select t.f22, f02, min(f_date) as f_date, max(l_date) as l_date,f06,f07\n",
    "     from\n",
    "    ( select * from tb_test_num_tmp\n",
    "    union all\n",
    "    select * from tb_test_num ) t\n",
    "    group by t.f22,f02,f06,f07\n",
    ";\"\"\"\n",
    "\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \"drop table if exists sample.tb_test_num;\"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \" alter table sample.tb_test_num_tmp1 rename to sample.tb_test_num; \"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "sqlContext.sql(query) \n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "\n",
    "query = \" select * from sample.tb_test_num; \"\n",
    "logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "runstarttime = datetime.now()\n",
    "df = sqlContext.sql(query) \n",
    "df.show(100, False)\n",
    "runfinishtime = datetime.now()\n",
    "logging.info(\"Finished Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
