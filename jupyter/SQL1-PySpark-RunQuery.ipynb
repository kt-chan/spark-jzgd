{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install SQLAlchemy==1.3\n",
    "!pip install pyhive[hive]\n",
    "!pip install thrift\n",
    "!pip install sasl\n",
    "!pip install thrift-sasl \n",
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/**\n",
    "注意：\n",
    "@date1表示开始时间，@date2表示结束时间，一般两者相差一天，即取一天的数据。\n",
    "bd/ad/ld/cp格式都是yyyymmddhh24，比如2023032301\n",
    "@batchId表示批次的变量，yyyymmddhh24格式，比如2023032301\n",
    "udf_is_null 自定义函数，用于判断字段是否为空，如果字段为空/null或者空字符串，都返回0，否则返回1\n",
    "--主表建表语句 TABLE tb_test --每天1T数据，f02本端号码（去重后2千万），f04对端号码（去重后2千万），f16中文内容\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys,os,logging\n",
    "import argparse\n",
    "\n",
    "\n",
    "date_strftime_format = \"%d-%b-%y %H:%M:%S\"\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='[%(asctime)s] %(name)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--f\", \"--filepath\", type=str, default=\"./conf/default.conf\", help='provide configuration filepath')\n",
    "args = parser.parse_args(args=['--filepath', './conf/default.conf'])\n",
    "configFilePath = args.f\n",
    "\n",
    "from configparser import ConfigParser\n",
    "config_object = ConfigParser()\n",
    "config_object.read(configFilePath)\n",
    "scaleinfo = config_object[\"SCALEINFO\"]\n",
    "scale = scaleinfo.getint(\"scale_gb\")\n",
    "batch = scaleinfo.getint(\"batch_k\")\n",
    "timespan_days = scaleinfo.getint(\"timespan_days\")\n",
    "droptable = scaleinfo.getboolean(\"droptable\")\n",
    "\n",
    "logging.info(\"Using following scale configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SCALEINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)\n",
    "\n",
    "    \n",
    "systeminfo = config_object[\"SYSTEMINFO\"]\n",
    "SPARK_APP_NAME = str(systeminfo.get(\"spark.app.name\")).strip('\\\"')\n",
    "SPARK_MASTER = str(systeminfo.get(\"spark.master.hostpath\")).strip('\\\"')\n",
    "HIVE_HMS_HOST= str(systeminfo.get(\"hive.metastore.uris\")).strip('\\\"')\n",
    "SPARK_WAREHOUSE_DIR = str(systeminfo.get(\"spark.sql.warehouse.dir\")).strip('\\\"')\n",
    "SPARK_DRIVER_CORES = systeminfo.getint(\"spark_driver_cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.driver.memory\")).strip('\\\"')\n",
    "SPARK_EXECUTOR_CORES = systeminfo.getint(\"spark.executor.cores\")\n",
    "SPARK_DRIVER_MEMORY = str(systeminfo.get(\"spark.executor.memory\")).strip('\\\"')\n",
    "\n",
    "logging.info(\"Using following system configuration: \")\n",
    "for (each_key, each_val) in config_object.items(config_object[\"SYSTEMINFO\"].name):\n",
    "    logging.info( each_key + \":\" + each_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-19 14:15:51,246] numexpr.utils {utils.py:141} INFO - NumExpr defaulting to 8 threads.\n",
      "[2023-05-19 14:16:04,588] root {<ipython-input-2-86992297f6c8>:26} INFO - Spark Version: 3.3.1\n",
      "[2023-05-19 14:16:04,595] root {<ipython-input-2-86992297f6c8>:27} INFO - PySpark Version: 3.3.1\n",
      "[2023-05-19 14:16:04,599] root {<ipython-input-2-86992297f6c8>:28} INFO - Pandas Version: 1.3.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions  import from_unixtime\n",
    "from time import sleep\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(SPARK_APP_NAME) \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"hive.metastore.uris\", HIVE_HMS_HOST) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", SPARK_WAREHOUSE_DIR) \\\n",
    "        .config(\"spark_driver_cores\", SPARK_DRIVER_CORES) \\\n",
    "        .config(\"spark.driver.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .config(\"spark.executor.cores\", SPARK_EXECUTOR_CORES) \\\n",
    "        .config(\"spark.executor.memory\", SPARK_DRIVER_MEMORY) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "sqlContext = SQLContext(spark.sparkContext, sparkSession=spark)\n",
    "spark.sparkContext.version\n",
    "logging.info(\"Spark Version: \" + spark.version)\n",
    "logging.info(\"PySpark Version: \" + pyspark.__version__)\n",
    "logging.info(\"Pandas Version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_view_tb_test_qf_tmp1(batchId, dateFrom, dateTo):\n",
    "    \n",
    "    logging.info(\"CREATE OR REPLACE TEMPORARY VIEW tb_test_qf_tmp1  ...\")\n",
    "\n",
    "    sqlContext.sql(\"use sample;\") \n",
    "    query = \"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW tb_test_qf_tmp1 \n",
    "        as\n",
    "        with tb_test_qf_tmp as (\n",
    "            select trim(f02) as f02, trim(f04) as f04,trim(f22) as f22,\n",
    "            regexp_replace(regexp_replace(regexp_replace(trim(f16),'\\\\\\?+','\\?') ,'0+','0'),'[ \\t]+',' ') as f16,\n",
    "            from_unixtime(unix_timestamp(cast(f30 as string),'yyyyMMddHH'), 'yyyy-MM-dd-HH')  as bd,\n",
    "            {batchId} as ad,\n",
    "            trim(f06) as f06,trim(f07) as f07\n",
    "            from tb_test t\n",
    "            where f31 >= {date1} and f31 < {date2} and f14 = '49'\n",
    "        ),\n",
    "        tb_test_qf_tmp1 as (\n",
    "        select a.f22,a.f02,a.f16, count(distinct a.f04) as cnt,a.bd ,a.ad,\n",
    "        a.f06,a.f07\n",
    "        from tb_test_qf_tmp a\n",
    "        left join tb_sev_u b on a.f02 = b.id\n",
    "        where a.f02 is not null and b.id is null\n",
    "        and length(a.f16) > 10\n",
    "        group by a.f22,a.f02,a.f16,a.bd,a.ad,a.f06,a.f07 \n",
    "        having cnt > 10\n",
    "        )\n",
    "        select * from tb_test_qf_tmp1\n",
    "        ;\n",
    "    \"\"\".format(batchId=batchId, date1=dateFrom, date2=dateTo)\n",
    "\n",
    "    logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "    sqlContext.sql(query) \n",
    "    logging.info(\"View tb_test_qf_tmp1 created.\")\n",
    "\n",
    "    \n",
    "def getLastBatchDate():\n",
    "    sqlContext.sql(\"use sample\");\n",
    "    query=\"select max(batchId) as lastdate from sample.tb_test_qf_stat_log\"\n",
    "    logging.info(\"Executing query: \" + query)\n",
    "    lastBatch =(int(sqlContext.sql(query).collect()[0][\"lastdate\"]))\n",
    "    logging.info(\"Completed query: \" + query + \" with latchBatch = \" + str(lastBatch) + \".\")\n",
    "    return lastBatch\n",
    "\n",
    "def reinitTable(batchId):\n",
    "    sqlContext.sql(\"use sample\");\n",
    "    query = \"alter table tb_test_qf_stat drop if exists partition (ad = {batchId})\".format(batchId=batchId)\n",
    "    logging.info(\"Executing query: \" + query)\n",
    "    sqlContext.sql(query);\n",
    "    logging.info(\"Completed query: \" + query + \" with latchBatch = \" + str(batchId) + \".\")\n",
    "    return True\n",
    "\n",
    "\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "lastBatch = getLastBatchDate()\n",
    "lastBatchDate = datetime.strptime(str(lastBatch), \"%Y%m%d%H\")\n",
    "dayrange = 1\n",
    "date1  = lastBatchDate\n",
    "date2  = lastBatchDate + relativedelta(days=dayrange)\n",
    "\n",
    "dateFrom = date1.strftime(\"%Y%m%d%H\")\n",
    "dateTo = date2.strftime(\"%Y%m%d%H\")\n",
    "rounds = math.ceil(timedelta(days=dayrange)  / timedelta(hours=1))\n",
    "\n",
    "logging.info(\"Running total \" + str(rounds) + \" rounds from \" + dateFrom + \" to \" + dateTo)\n",
    "\n",
    "for i in range(rounds):\n",
    "    if i > 0 :\n",
    "        date1 = (date1 + relativedelta(hours=1))\n",
    "    \n",
    "    date2 = (date1 + relativedelta(hours=1))\n",
    "    batchId = date2.strftime(\"%Y%m%d%H\")\n",
    "    reinitTable(batchId)\n",
    "\n",
    "date1  = lastBatchDate\n",
    "date2  = lastBatchDate + relativedelta(days=dayrange)\n",
    "runstarttime = datetime.now()\n",
    "for i in range(rounds):\n",
    "    if i > 0 :\n",
    "        date1 = (date1 + relativedelta(hours=1))\n",
    "    \n",
    "    date2 = (date1 + relativedelta(hours=1))\n",
    "    batchId = date2.strftime(\"%Y%m%d%H\")\n",
    "    dateFrom = date1.strftime(\"%Y%m%d%H\")\n",
    "    dateTo = date2.strftime(\"%Y%m%d%H\")\n",
    "    \n",
    "    logging.info(\"Running rounds \" + str(i+1) + \" for batchId =\" + str(batchId) + \".\")\n",
    "    create_view_tb_test_qf_tmp1(batchId, dateFrom, dateTo)\n",
    "    \n",
    "    query = \"alter table tb_test_qf_stat drop if exists partition (ad = {batchId})\".format(batchId=batchId)\n",
    "    logging.info(\"Executing query: \" + query)\n",
    "    sqlContext.sql(query);\n",
    "    logging.info(\"table sample.tb_test_qf_stat partition \" + str(batchId) + \" dropped.\")\n",
    "    \n",
    "    sqlContext.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "    query = \"\"\"\n",
    "     insert into sample.tb_test_qf_stat partition(bd, ad)\n",
    "     select f22,f02,f16,cnt,f06,f07,bd,ad\n",
    "     from tb_test_qf_tmp1\n",
    "    ;\"\"\"\n",
    "    logging.info(\"\\nExecuting query: \\n\" + query)\n",
    "    runstarttime = datetime.now()\n",
    "    sqlContext.sql(query) \n",
    "    runfinishtime = datetime.now()\n",
    "    logging.info(\"finished round \" + str(i+1) + \" with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")\n",
    "    \n",
    "    query=\"\"\"\n",
    "        select f22,f02,f16,cnt,bd,f06,f07 from\n",
    "        ( select *, row_number() over(partition by f02 order by bd desc ) rn from tb_test_qf_stat) t\n",
    "        where t.rn =1;\n",
    "    \"\"\"\n",
    "    df=sqlContext.sql(query) \n",
    "    df.show(100,False)\n",
    "    \n",
    "    spark.catalog.clearCache()\n",
    "    gc.collect()\n",
    "\n",
    "runfinishtime = datetime.now()\n",
    "sqlContext.sql(\"set hive.exec.dynamic.partition.mode=strict\")\n",
    "logging.info(\"Finished tb_test_qf_stat Insert Query with \" + str( (runfinishtime - runstarttime).seconds)  + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = sqlContext.sql(\"select * from sample.tb_test_qf_stat\")\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
